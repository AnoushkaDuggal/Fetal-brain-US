{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a16ab453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Ready to process: /home/fetalusr1/Fetal-Head-Segmentation-master/IMG_20250329_13_1.nii\n",
      "üíæ Results will save to: ./FilteredRes/segmentation_result_13_1_interpolated.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# GLOBAL CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "# Change this path whenever you have a new sample\n",
    "FILE_ID = \"13_1\"  \n",
    "IMAGE_PATH = f\"/home/fetalusr1/Fetal-Head-Segmentation-master/IMG_20250329_13_1.nii\"\n",
    "\n",
    "# This automatically names your outputs based on the input\n",
    "MASK_OUTPUT_PATH = f\"./FilteredRes/segmentation_result_{FILE_ID}_interpolated.nii.gz\"\n",
    "REPORT_ZIP_NAME = f\"3D_Full_Report_{FILE_ID}.zip\"\n",
    "\n",
    "print(f\"üöÄ Ready to process: {IMAGE_PATH}\")\n",
    "print(f\"üíæ Results will save to: {MASK_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d97dfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory fragmentation rules applied.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# optimizing memory allocation to reduce fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "print(\"‚úÖ Memory fragmentation rules applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2db9a473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fetalusr1/miniconda3/envs/fetal_project/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory settings applied. Free memory: 45.71 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# 1. Help PyTorch manage fragmented memory\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# 2. Clear any lingering cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"‚úÖ Memory settings applied. Free memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f6bfc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Forced Library Path: /home/fetalusr1/miniconda3/envs/fetal_project/lib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the path to your current environment\n",
    "conda_prefix = sys.prefix\n",
    "lib_path = os.path.join(conda_prefix, 'lib')\n",
    "\n",
    "# Force this path to the front of the line\n",
    "os.environ['LD_LIBRARY_PATH'] = f\"{lib_path}:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "\n",
    "print(f\"‚úÖ Forced Library Path: {lib_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a7fdd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "üéâ SUCCESS: GPU Math works! Result: [3. 8.]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "try:\n",
    "    # Try a simple calculation on the GPU\n",
    "    x = torch.tensor([1.0, 2.0]).cuda()\n",
    "    y = torch.tensor([3.0, 4.0]).cuda()\n",
    "    z = x * y\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"üéâ SUCCESS: GPU Math works! Result: {z.cpu().numpy()}\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "except RuntimeError as e:\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"‚ùå FAILURE: {e}\")\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c686b2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Authorization required, but no authorization protocol specified\n",
      "\n",
      "Authorization required, but no authorization protocol specified\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deformable Transformer Encoder is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fetalusr1/miniconda3/envs/fetal_project/lib/python3.8/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from modeling.BaseModel import BaseModel\n",
    "from modeling import build_model\n",
    "from utilities.distributed import init_distributed\n",
    "from utilities.arguments import load_opt_from_config_files\n",
    "from utilities.constants import BIOMED_CLASSES\n",
    "import matplotlib.pyplot as plt\n",
    "from inference_utils.inference import interactive_infer_image\n",
    "from inference_utils.output_processing import check_mask_stats\n",
    "from inference_utils.processing_utils import process_intensity_image\n",
    "from inference_utils.processing_utils import read_nifti\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import SimpleITK as sitk\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "out_probs = []\n",
    "predicted_masks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9f2c29",
   "metadata": {},
   "source": [
    "## Loading the Finetuned BiomedParse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b84965ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$UNUSED$ criterion.empty_weight, Ckpt Shape: torch.Size([17])\n"
     ]
    }
   ],
   "source": [
    "# Build model config\n",
    "opt = load_opt_from_config_files([\"configs/biomedparse_inference.yaml\"])\n",
    "opt = init_distributed(opt)\n",
    "\n",
    "# Load model from pretrained weights\n",
    "finetuned_pth = '/home/fetalusr1/Fetal-Head-Segmentation-master/model_state_dict.pt' # Replace with the path to your finetuned checkpoint\n",
    "\n",
    "model = BaseModel(opt, build_model(opt)).from_pretrained(pretrained=finetuned_pth).eval().cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.model.sem_seg_head.predictor.lang_encoder.get_text_embeddings(BIOMED_CLASSES + [\"background\"], is_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc46355",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7089a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation_masks(original_image, segmentation_masks, texts, rotate=0):\n",
    "    ''' Plot a list of segmentation mask over an image showing only the segmented region.\n",
    "    '''\n",
    "    original_image = original_image[:, :, :3]\n",
    "\n",
    "    segmented_images = []\n",
    "\n",
    "    for i, mask in enumerate(segmentation_masks):\n",
    "        segmented_image = original_image.copy()\n",
    "        segmented_image[mask <= 0.5] = [0, 0, 0]\n",
    "        segmented_images.append(segmented_image)\n",
    "        \n",
    "    return segmented_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d04f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_nifti(file_path, text_prompts, is_CT, slice_idx, site=None, HW_index=(0, 1), channel_idx=None, rotate=0):\n",
    "\n",
    "    image = read_nifti(file_path, is_CT, slice_idx, site=site, HW_index=HW_index, channel_idx=channel_idx)\n",
    "    \n",
    "    pred_mask,out_prob = interactive_infer_image(model, Image.fromarray(image), text_prompts)\n",
    "    predicted_masks.append(pred_mask)\n",
    "    segmented_images = get_segmentation_masks(image, pred_mask, text_prompts, rotate=rotate)\n",
    "    out_probs.append(out_prob)\n",
    "    \n",
    "    return image, pred_mask, segmented_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7571fe0b",
   "metadata": {},
   "source": [
    "### Post-processing Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3d7a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predicted_volume(volume_data, threshold_factor=0.35, output_prefix='processed'):\n",
    "    \"\"\"\n",
    "    Process the predicted volume to filter based on ellipse measurements.\n",
    "    \"\"\"\n",
    "    data = volume_data\n",
    "    print(f\"Processing volume with shape: {data.shape}\")\n",
    "    \n",
    "    # Calculate measurements for all slices\n",
    "    results = []\n",
    "    z_0 = data.shape[2] // 2  # Reference slice (middle slice)\n",
    "    \n",
    "    print(f\"Reference slice: {z_0}\")\n",
    "    \n",
    "    for i in range(data.shape[2]):\n",
    "        slice_data = data[:, :, i]\n",
    "        \n",
    "        # Skip empty slices\n",
    "        if np.sum(slice_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Binarize the slice\n",
    "        slice_bin = np.where(slice_data > 0, 1, 0).astype(np.uint8)\n",
    "        \n",
    "        # Fill holes\n",
    "        slice_bin_filled = sitk.BinaryFillhole(sitk.GetImageFromArray(slice_bin))\n",
    "        slice_bin_filled = sitk.GetArrayFromImage(slice_bin_filled)\n",
    "        \n",
    "        # Get region properties\n",
    "        labeled_image = label(slice_bin_filled)\n",
    "        props = regionprops(labeled_image)\n",
    "        \n",
    "        for prop in props:\n",
    "            results.append({\n",
    "                'slice_index': i,\n",
    "                'major_axis_length': prop.major_axis_length,\n",
    "                'minor_axis_length': prop.minor_axis_length,\n",
    "                'centroid_x': prop.centroid[1],\n",
    "                'centroid_y': prop.centroid[0],\n",
    "                'orientation': prop.orientation,\n",
    "                'area': prop.area\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(f\"Found {len(results)} regions across {len(df_results['slice_index'].unique())} slices\")\n",
    "    \n",
    "    # Get reference slice measurements for filtering\n",
    "    standard_slice_data = df_results[df_results['slice_index'] == z_0]\n",
    "    \n",
    "    if standard_slice_data.empty:\n",
    "        print(f\"Warning: No data found in reference slice {z_0}\")\n",
    "        # Use overall median as fallback\n",
    "        major_axis_length_std = df_results['major_axis_length'].median()\n",
    "        minor_axis_length_std = df_results['minor_axis_length'].median()\n",
    "        centroid_x_std = df_results['centroid_x'].median()\n",
    "        centroid_y_std = df_results['centroid_y'].median()\n",
    "    else:\n",
    "        major_axis_length_std = standard_slice_data['major_axis_length'].values[0]\n",
    "        minor_axis_length_std = standard_slice_data['minor_axis_length'].values[0]\n",
    "        centroid_x_std = standard_slice_data['centroid_x'].values[0]\n",
    "        centroid_y_std = standard_slice_data['centroid_y'].values[0]\n",
    "    \n",
    "    # Define thresholds\n",
    "    major_axis_length_threshold = major_axis_length_std * (1 - threshold_factor)\n",
    "    minor_axis_length_threshold = minor_axis_length_std * (1 - threshold_factor)\n",
    "    \n",
    "    print(f\"Reference measurements - Major: {major_axis_length_std:.2f}, Minor: {minor_axis_length_std:.2f}\")\n",
    "    print(f\"Filtering thresholds - Major: {major_axis_length_threshold:.2f}, Minor: {minor_axis_length_threshold:.2f}\")\n",
    "    \n",
    "    # Filter based on thresholds\n",
    "    filtered_df = df_results[\n",
    "        (df_results['major_axis_length'] >= major_axis_length_threshold) &\n",
    "        (df_results['minor_axis_length'] >= minor_axis_length_threshold)\n",
    "    ]\n",
    "    \n",
    "    print(f\"After filtering: {len(filtered_df)} regions in {len(filtered_df['slice_index'].unique())} slices\")\n",
    "    \n",
    "    # In filtered_df, in case of repeated slices, keep the one with maximum major axis length\n",
    "    filtered_df = filtered_df.loc[filtered_df.groupby('slice_index')['major_axis_length'].idxmax()]\n",
    "    \n",
    "    # Create filtered volume\n",
    "    filtered_slices = filtered_df['slice_index'].unique()\n",
    "    filtered_volume = np.zeros_like(data)\n",
    "    \n",
    "    for slice_idx in range(data.shape[2]):\n",
    "        if slice_idx in filtered_slices:\n",
    "            filtered_volume[:, :, slice_idx] = data[:, :, slice_idx]\n",
    "    \n",
    "    return filtered_volume, filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46da5a2",
   "metadata": {},
   "source": [
    "### Interpolation Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "164a72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_blank_slices(image_path, processed_volume, blank_slices, predicted_masks, delta=1):\n",
    "    \"\"\"\n",
    "    Interpolate blank slices in the processed volume using the previous slice.\n",
    "    \"\"\"\n",
    "    vol_data = nib.load(image_path).get_fdata()\n",
    "    central_slice = vol_data.shape[2] // 2\n",
    "    \n",
    "    for slice_idx in blank_slices:\n",
    "        # Ensure we have a valid previous slice\n",
    "        prev_slice_idx = slice_idx - delta\n",
    "        if prev_slice_idx < 0 or prev_slice_idx >= len(predicted_masks):\n",
    "            continue\n",
    "            \n",
    "        # Get the previous mask\n",
    "        prev_mask = predicted_masks[prev_slice_idx][0]  # Get first mask from the list\n",
    "        \n",
    "        #update predicted_masks\n",
    "        predicted_masks[slice_idx] = [prev_mask.copy()]  # Store the previous mask\n",
    "        # Ensure the previous mask is not empty\n",
    "        if np.sum(prev_mask) == 0:\n",
    "            print(f\"Warning: Previous mask for slice {prev_slice_idx} is empty. Skipping interpolation for slice {slice_idx}.\")\n",
    "            continue\n",
    "        # Scale the mask based on position relative to center\n",
    "        if slice_idx < central_slice: \n",
    "            # Increase the mask size by 0.5%\n",
    "            new_mask = prev_mask * 1.005\n",
    "        else:\n",
    "            # Decrease the mask size by 0.5%\n",
    "            new_mask = prev_mask * 0.995\n",
    "        \n",
    "        # Read the original image for this slice\n",
    "        image = read_nifti(image_path, is_CT=False, slice_idx=slice_idx, site=None, HW_index=(0, 1), channel_idx=None)\n",
    "        \n",
    "        # Get the segmented image\n",
    "        new_segmented_image = get_segmentation_masks(image, [new_mask], ['fetal head'], rotate=0)[0]\n",
    "        \n",
    "        # Convert RGB segmentation to grayscale if needed\n",
    "        if len(new_segmented_image.shape) == 3:\n",
    "            gray_mask = np.mean(new_segmented_image, axis=2)\n",
    "        else:\n",
    "            gray_mask = new_segmented_image\n",
    "        \n",
    "        # Resize to match volume dimensions and store\n",
    "        from skimage.transform import resize\n",
    "        processed_volume[:, :, slice_idx] = resize(gray_mask, (vol_data.shape[0], vol_data.shape[1]), preserve_range=True)\n",
    "    \n",
    "    return processed_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcfe588",
   "metadata": {},
   "source": [
    "## Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d578d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227, 149, 234)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = '/home/fetalusr1/Fetal-Head-Segmentation-master/IMG_20250329_13_1.nii'\n",
    "text_prompt = ['fetal head']\n",
    "vol = nib.load(image_path)\n",
    "vol_data = vol.get_fdata()\n",
    "vol_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad684da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fetalusr1/miniconda3/envs/fetal_project/lib/python3.8/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  max_size = (max_size + (stride - 1)) // stride * stride\n",
      "/home/fetalusr1/Fetal-Head-Segmentation-master/modeling/modules/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
      "/home/fetalusr1/miniconda3/envs/fetal_project/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing volume with shape: (227, 149, 234)\n",
      "Reference slice: 117\n",
      "Found 260 regions across 234 slices\n",
      "Reference measurements - Major: 168.25, Minor: 95.13\n",
      "Filtering thresholds - Major: 100.95, Minor: 57.08\n",
      "After filtering: 109 regions in 109 slices\n",
      "Original volume had 1379858 non-zero voxels\n",
      "Processed volume has 1131293 non-zero voxels\n"
     ]
    }
   ],
   "source": [
    "# Initialize volume to store all prediction masks\n",
    "pred_volume = np.zeros((vol_data.shape[0], vol_data.shape[1], vol_data.shape[2]))\n",
    "\n",
    "counter = 0\n",
    "for slice_idx in range(vol_data.shape[2]):\n",
    "    image, pred_mask, segmentation_mask = inference_nifti(image_path, text_prompt, is_CT=False, slice_idx=slice_idx, site=None, rotate=0)\n",
    "    \n",
    "    # Convert RGB segmentation mask to grayscale\n",
    "    if len(segmentation_mask[0].shape) == 3:\n",
    "        # Convert to grayscale by taking the mean across color channels\n",
    "        gray_mask = np.mean(segmentation_mask[0], axis=2)\n",
    "    else:\n",
    "        gray_mask = segmentation_mask[0]\n",
    "    \n",
    "    # Store the prediction mask in the volume\n",
    "    pred_volume[:, :, slice_idx] = resize(gray_mask, (vol_data.shape[0], vol_data.shape[1]), preserve_range=True)\n",
    "\n",
    "# Post processing\n",
    "\n",
    "processed_volume, filtered_measurements = process_predicted_volume(\n",
    "    pred_volume, \n",
    "    threshold_factor=0.4,  # Adjust as needed\n",
    "    output_prefix='3_2'\n",
    ")\n",
    "\n",
    "print(f\"Original volume had {np.sum(pred_volume > 0)} non-zero voxels\")\n",
    "print(f\"Processed volume has {np.sum(processed_volume > 0)} non-zero voxels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ca1e0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First filtered slice: 72\n",
      "Last filtered slice: 228\n",
      "Blank slices from 72 to 233: [144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227]\n"
     ]
    }
   ],
   "source": [
    "#Get the first slice that survived filtering\n",
    "first_filtered_slice = min(filtered_measurements['slice_index'].unique())\n",
    "last_filtered_slice = max(filtered_measurements['slice_index'].unique())\n",
    "print(f\"First filtered slice: {first_filtered_slice}\")\n",
    "print(f\"Last filtered slice: {last_filtered_slice}\")\n",
    "#from the filtered slice to the center slice, get all the slices which are blank\n",
    "blank_slices = []\n",
    "for slice_idx in range(first_filtered_slice, last_filtered_slice + 1):\n",
    "    if np.sum(processed_volume[:, :, slice_idx]) == 0:\n",
    "        blank_slices.append(slice_idx)\n",
    "# Print the blank slices\n",
    "print(f\"Blank slices from {first_filtered_slice} to {vol_data.shape[2]-1}: {blank_slices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "325c6bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw prediction saved to ./results/segmentation_RAW.nii.gz\n",
      "Processed prediction saved to ./FilteredRes/segmentation_fil.nii.gz\n",
      "Interpolated prediction saved to ./FilteredRes/segmentation_inter.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create results directories if they don't exist\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "os.makedirs('./FilteredRes', exist_ok=True)\n",
    "\n",
    "# Load original NIfTI for header info\n",
    "original_nii = nib.load(image_path)\n",
    "\n",
    "# Save raw prediction\n",
    "pred_nii = nib.Nifti1Image(pred_volume, original_nii.affine, original_nii.header)\n",
    "raw_filename = f'./results/segmentation_RAW.nii.gz'\n",
    "nib.save(pred_nii, raw_filename)\n",
    "print(f\"Raw prediction saved to {raw_filename}\")\n",
    "\n",
    "# Save processed prediction\n",
    "processed_nii = nib.Nifti1Image(processed_volume, original_nii.affine, original_nii.header)\n",
    "processed_filename = f'./FilteredRes/segmentation_fil.nii.gz'\n",
    "nib.save(processed_nii, processed_filename)\n",
    "print(f\"Processed prediction saved to {processed_filename}\")\n",
    "\n",
    "interpolated_volume = interpolate_blank_slices(image_path, processed_volume, blank_slices, predicted_masks, delta=1)\n",
    "# Save interpolated prediction\n",
    "interpolated_nii = nib.Nifti1Image(interpolated_volume, original_nii.affine, original_nii.header)\n",
    "interpolated_filename = f'./FilteredRes/segmentation_inter.nii.gz'\n",
    "nib.save(interpolated_nii, interpolated_filename)\n",
    "print(f\"Interpolated prediction saved to {interpolated_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c17ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ°Ô∏è Expanding mask buffer...\n",
      "‚úÖ Anatomically Preserved Result Saved: ./FilteredRes/13_1_PRESERVED.nii.gz\n"
     ]
    }
   ],
   "source": [
    "'''import scipy.ndimage as ndimage\n",
    "\n",
    "# 1. Expand the mask slightly (Dilation)\n",
    "# This adds a 2-3 pixel buffer around the head so we don't cut into the skull\n",
    "print(\"üõ°Ô∏è Expanding mask buffer...\")\n",
    "structure = ndimage.generate_binary_structure(3, 1)\n",
    "# dilate by 2 iterations to ensure outer skull is included\n",
    "expanded_mask = ndimage.binary_dilation(interpolated_volume > 0, structure=structure, iterations=2)\n",
    "\n",
    "# 2. Smooth the expanded mask (to keep it natural, not jagged)\n",
    "expanded_mask = ndimage.median_filter(expanded_mask.astype(np.float32), size=3)\n",
    "\n",
    "# 3. Multiply by Original Intensity\n",
    "# Now we use the expanded mask so we don't strip too much\n",
    "final_output_vol = vol_data * expanded_mask\n",
    "\n",
    "# 4. Save with Header Correction (Fixing the Squished look)\n",
    "original_zooms = original_nii.header.get_zooms()\n",
    "improved_nii = nib.Nifti1Image(final_output_vol, original_nii.affine, original_nii.header)\n",
    "improved_nii.header.set_zooms(original_zooms)\n",
    "\n",
    "improved_filename = f'./FilteredRes/{FILE_ID}_PRESERVED.nii.gz'\n",
    "nib.save(improved_nii, improved_filename)\n",
    "print(f\"‚úÖ Anatomically Preserved Result Saved: {improved_filename}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2de6b915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG_20250329_1_1.nii   IMG_20250329_15_1.nii  IMG_20250329_5_2.nii\n",
      "IMG_20250329_11.nii    IMG_20250329_15_2.nii  IMG_20250329_5_3.nii\n",
      "IMG_20250329_12_1.nii  IMG_20250329_2_1.nii   IMG_20250329_5_4.nii\n",
      "IMG_20250329_12_2.nii  IMG_20250329_2_2.nii   IMG_20250329_6_1.nii\n",
      "IMG_20250329_1_2.nii   IMG_20250329_2_3.nii   IMG_20250329_6_2.nii\n",
      "IMG_20250329_13_1.nii  IMG_20250329_3_1.nii   IMG_20250329_7_1.nii\n",
      "IMG_20250329_13_2.nii  IMG_20250329_3_2.nii   IMG_20250329_7_2.nii\n",
      "IMG_20250329_13_3.nii  IMG_20250329_3_3.nii   IMG_20250329_7_3.nii\n",
      "IMG_20250329_1_3.nii   IMG_20250329_3.nii     IMG_20250329_8.nii\n",
      "IMG_20250329_14_1.nii  IMG_20250329_4_1.nii   IMG_20250329_9_1.nii\n",
      "IMG_20250329_14_2.nii  IMG_20250329_4_2.nii   IMG_20250329_9_2.nii\n",
      "IMG_20250329_14_3.nii  IMG_20250329_5_1.nii   IMG_20250329_9_3.nii\n"
     ]
    }
   ],
   "source": [
    "!ls Param/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99670582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ°Ô∏è Applying conservative dilation (2 iterations)...\n",
      "‚úÖ Success. Saved with conservative dilation to: ./FilteredRes/FINAL_2ndTIVE_13_1.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import scipy.ndimage as ndimage\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "# 1. CREATE MASK FROM EXISTING WORKING VOLUME\n",
    "# We start with 'interpolated_volume' which you confirmed is correct/working in memory.\n",
    "# Any pixel with data becomes part of the mask.\n",
    "current_mask = interpolated_volume > 0\n",
    "\n",
    "# 2. CONSERVATIVE DILATION (The Skull Fix)\n",
    "# Iterations=2: Expands the mask just enough to touch the inner skull without overshooting.\n",
    "print(\"üõ°Ô∏è Applying conservative dilation (2 iterations)...\")\n",
    "structure = ndimage.generate_binary_structure(3, 1)\n",
    "dilated_mask = ndimage.binary_dilation(current_mask, structure=structure, iterations=3)\n",
    "\n",
    "# Optional: Slight smoothing to keep edges organic\n",
    "dilated_mask = ndimage.median_filter(dilated_mask.astype(np.float32), size=3)\n",
    "\n",
    "# 3. APPLY TO ORIGINAL TEXTURE\n",
    "# Use the dilated mask to grab the brain + skull edge from the original data\n",
    "final_output = vol_data * dilated_mask\n",
    "\n",
    "# 4. SAVE USING NOTEBOOK LOGIC (No 'set_zooms' override)\n",
    "# We use the exact header/affine from 'original_nii' just like your working Cell 96.\n",
    "final_nii = nib.Nifti1Image(final_output.astype(np.float32), original_nii.affine, original_nii.header)\n",
    "\n",
    "save_path = f'./FilteredRes/{FILE_ID}_Corrected.nii.gz'\n",
    "nib.save(final_nii, save_path)\n",
    "print(f\"‚úÖ Success. Saved with correction: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fetal_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
