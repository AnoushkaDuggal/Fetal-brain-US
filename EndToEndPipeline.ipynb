{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d97dfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory fragmentation rules applied.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# optimizing memory allocation to reduce fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "print(\"‚úÖ Memory fragmentation rules applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2db9a473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fetalusr1/miniconda3/envs/fetal_project/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory settings applied. Free memory: 3.29 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# 1. Help PyTorch manage fragmented memory\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# 2. Clear any lingering cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"‚úÖ Memory settings applied. Free memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f6bfc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Forced Library Path: /home/fetalusr1/miniconda3/envs/fetal_project/lib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the path to your current environment\n",
    "conda_prefix = sys.prefix\n",
    "lib_path = os.path.join(conda_prefix, 'lib')\n",
    "\n",
    "# Force this path to the front of the line\n",
    "os.environ['LD_LIBRARY_PATH'] = f\"{lib_path}:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "\n",
    "print(f\"‚úÖ Forced Library Path: {lib_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a7fdd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "üéâ SUCCESS: GPU Math works! Result: [3. 8.]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "try:\n",
    "    # Try a simple calculation on the GPU\n",
    "    x = torch.tensor([1.0, 2.0]).cuda()\n",
    "    y = torch.tensor([3.0, 4.0]).cuda()\n",
    "    z = x * y\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"üéâ SUCCESS: GPU Math works! Result: {z.cpu().numpy()}\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "except RuntimeError as e:\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"‚ùå FAILURE: {e}\")\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c686b2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fetalusr1/miniconda3/envs/fetal_project/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Authorization required, but no authorization protocol specified\n",
      "\n",
      "Authorization required, but no authorization protocol specified\n",
      "\n",
      "Authorization required, but no authorization protocol specified\n",
      "\n",
      "Authorization required, but no authorization protocol specified\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deformable Transformer Encoder is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fetalusr1/miniconda3/envs/fetal_project/lib/python3.8/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from modeling.BaseModel import BaseModel\n",
    "from modeling import build_model\n",
    "from utilities.distributed import init_distributed\n",
    "from utilities.arguments import load_opt_from_config_files\n",
    "from utilities.constants import BIOMED_CLASSES\n",
    "import matplotlib.pyplot as plt\n",
    "from inference_utils.inference import interactive_infer_image\n",
    "from inference_utils.output_processing import check_mask_stats\n",
    "from inference_utils.processing_utils import process_intensity_image\n",
    "from inference_utils.processing_utils import read_nifti\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import SimpleITK as sitk\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "out_probs = []\n",
    "predicted_masks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9f2c29",
   "metadata": {},
   "source": [
    "## Loading the Finetuned BiomedParse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b84965ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$UNUSED$ criterion.empty_weight, Ckpt Shape: torch.Size([17])\n"
     ]
    }
   ],
   "source": [
    "# Build model config\n",
    "opt = load_opt_from_config_files([\"configs/biomedparse_inference.yaml\"])\n",
    "opt = init_distributed(opt)\n",
    "\n",
    "# Load model from pretrained weights\n",
    "finetuned_pth = '/home/fetalusr1/Fetal-Head-Segmentation-master/model_state_dict.pt' # Replace with the path to your finetuned checkpoint\n",
    "\n",
    "model = BaseModel(opt, build_model(opt)).from_pretrained(pretrained=finetuned_pth).eval().cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.model.sem_seg_head.predictor.lang_encoder.get_text_embeddings(BIOMED_CLASSES + [\"background\"], is_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc46355",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7089a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation_masks(original_image, segmentation_masks, texts, rotate=0):\n",
    "    ''' Plot a list of segmentation mask over an image showing only the segmented region.\n",
    "    '''\n",
    "    original_image = original_image[:, :, :3]\n",
    "\n",
    "    segmented_images = []\n",
    "\n",
    "    for i, mask in enumerate(segmentation_masks):\n",
    "        segmented_image = original_image.copy()\n",
    "        segmented_image[mask <= 0.5] = [0, 0, 0]\n",
    "        segmented_images.append(segmented_image)\n",
    "        \n",
    "    return segmented_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d04f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_nifti(file_path, text_prompts, is_CT, slice_idx, site=None, HW_index=(0, 1), channel_idx=None, rotate=0):\n",
    "\n",
    "    image = read_nifti(file_path, is_CT, slice_idx, site=site, HW_index=HW_index, channel_idx=channel_idx)\n",
    "    \n",
    "    pred_mask,out_prob = interactive_infer_image(model, Image.fromarray(image), text_prompts)\n",
    "    predicted_masks.append(pred_mask)\n",
    "    segmented_images = get_segmentation_masks(image, pred_mask, text_prompts, rotate=rotate)\n",
    "    out_probs.append(out_prob)\n",
    "    \n",
    "    return image, pred_mask, segmented_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7571fe0b",
   "metadata": {},
   "source": [
    "### Post-processing Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d7a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predicted_volume(volume_data, threshold_factor=0.35, output_prefix='processed'):\n",
    "    \"\"\"\n",
    "    Process the predicted volume to filter based on ellipse measurements.\n",
    "    \"\"\"\n",
    "    data = volume_data\n",
    "    print(f\"Processing volume with shape: {data.shape}\")\n",
    "    \n",
    "    # Calculate measurements for all slices\n",
    "    results = []\n",
    "    z_0 = data.shape[2] // 2  # Reference slice (middle slice)\n",
    "    \n",
    "    print(f\"Reference slice: {z_0}\")\n",
    "    \n",
    "    for i in range(data.shape[2]):\n",
    "        slice_data = data[:, :, i]\n",
    "        \n",
    "        # Skip empty slices\n",
    "        if np.sum(slice_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Binarize the slice\n",
    "        slice_bin = np.where(slice_data > 0, 1, 0).astype(np.uint8)\n",
    "        \n",
    "        # Fill holes\n",
    "        slice_bin_filled = sitk.BinaryFillhole(sitk.GetImageFromArray(slice_bin))\n",
    "        slice_bin_filled = sitk.GetArrayFromImage(slice_bin_filled)\n",
    "        \n",
    "        # Get region properties\n",
    "        labeled_image = label(slice_bin_filled)\n",
    "        props = regionprops(labeled_image)\n",
    "        \n",
    "        for prop in props:\n",
    "            results.append({\n",
    "                'slice_index': i,\n",
    "                'major_axis_length': prop.major_axis_length,\n",
    "                'minor_axis_length': prop.minor_axis_length,\n",
    "                'centroid_x': prop.centroid[1],\n",
    "                'centroid_y': prop.centroid[0],\n",
    "                'orientation': prop.orientation,\n",
    "                'area': prop.area\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(f\"Found {len(results)} regions across {len(df_results['slice_index'].unique())} slices\")\n",
    "    \n",
    "    # Get reference slice measurements for filtering\n",
    "    standard_slice_data = df_results[df_results['slice_index'] == z_0]\n",
    "    \n",
    "    if standard_slice_data.empty:\n",
    "        print(f\"Warning: No data found in reference slice {z_0}\")\n",
    "        # Use overall median as fallback\n",
    "        major_axis_length_std = df_results['major_axis_length'].median()\n",
    "        minor_axis_length_std = df_results['minor_axis_length'].median()\n",
    "        centroid_x_std = df_results['centroid_x'].median()\n",
    "        centroid_y_std = df_results['centroid_y'].median()\n",
    "    else:\n",
    "        major_axis_length_std = standard_slice_data['major_axis_length'].values[0]\n",
    "        minor_axis_length_std = standard_slice_data['minor_axis_length'].values[0]\n",
    "        centroid_x_std = standard_slice_data['centroid_x'].values[0]\n",
    "        centroid_y_std = standard_slice_data['centroid_y'].values[0]\n",
    "    \n",
    "    # Define thresholds\n",
    "    major_axis_length_threshold = major_axis_length_std * (1 - threshold_factor)\n",
    "    minor_axis_length_threshold = minor_axis_length_std * (1 - threshold_factor)\n",
    "    \n",
    "    print(f\"Reference measurements - Major: {major_axis_length_std:.2f}, Minor: {minor_axis_length_std:.2f}\")\n",
    "    print(f\"Filtering thresholds - Major: {major_axis_length_threshold:.2f}, Minor: {minor_axis_length_threshold:.2f}\")\n",
    "    \n",
    "    # Filter based on thresholds\n",
    "    filtered_df = df_results[\n",
    "        (df_results['major_axis_length'] >= major_axis_length_threshold) &\n",
    "        (df_results['minor_axis_length'] >= minor_axis_length_threshold)\n",
    "    ]\n",
    "    \n",
    "    print(f\"After filtering: {len(filtered_df)} regions in {len(filtered_df['slice_index'].unique())} slices\")\n",
    "    \n",
    "    # In filtered_df, in case of repeated slices, keep the one with maximum major axis length\n",
    "    filtered_df = filtered_df.loc[filtered_df.groupby('slice_index')['major_axis_length'].idxmax()]\n",
    "    \n",
    "    # Create filtered volume\n",
    "    filtered_slices = filtered_df['slice_index'].unique()\n",
    "    filtered_volume = np.zeros_like(data)\n",
    "    \n",
    "    for slice_idx in range(data.shape[2]):\n",
    "        if slice_idx in filtered_slices:\n",
    "            filtered_volume[:, :, slice_idx] = data[:, :, slice_idx]\n",
    "    \n",
    "    return filtered_volume, filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46da5a2",
   "metadata": {},
   "source": [
    "### Interpolation Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "164a72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_blank_slices(image_path, processed_volume, blank_slices, predicted_masks, delta=1):\n",
    "    \"\"\"\n",
    "    Interpolate blank slices in the processed volume using the previous slice.\n",
    "    \"\"\"\n",
    "    vol_data = nib.load(image_path).get_fdata()\n",
    "    central_slice = vol_data.shape[2] // 2\n",
    "    \n",
    "    for slice_idx in blank_slices:\n",
    "        # Ensure we have a valid previous slice\n",
    "        prev_slice_idx = slice_idx - delta\n",
    "        if prev_slice_idx < 0 or prev_slice_idx >= len(predicted_masks):\n",
    "            continue\n",
    "            \n",
    "        # Get the previous mask\n",
    "        prev_mask = predicted_masks[prev_slice_idx][0]  # Get first mask from the list\n",
    "        \n",
    "        #update predicted_masks\n",
    "        predicted_masks[slice_idx] = [prev_mask.copy()]  # Store the previous mask\n",
    "        # Ensure the previous mask is not empty\n",
    "        if np.sum(prev_mask) == 0:\n",
    "            print(f\"Warning: Previous mask for slice {prev_slice_idx} is empty. Skipping interpolation for slice {slice_idx}.\")\n",
    "            continue\n",
    "        # Scale the mask based on position relative to center\n",
    "        if slice_idx < central_slice: \n",
    "            # Increase the mask size by 0.5%\n",
    "            new_mask = prev_mask * 1.005\n",
    "        else:\n",
    "            # Decrease the mask size by 0.5%\n",
    "            new_mask = prev_mask * 0.995\n",
    "        \n",
    "        # Read the original image for this slice\n",
    "        image = read_nifti(image_path, is_CT=False, slice_idx=slice_idx, site=None, HW_index=(0, 1), channel_idx=None)\n",
    "        \n",
    "        # Get the segmented image\n",
    "        new_segmented_image = get_segmentation_masks(image, [new_mask], ['fetal head'], rotate=0)[0]\n",
    "        \n",
    "        # Convert RGB segmentation to grayscale if needed\n",
    "        if len(new_segmented_image.shape) == 3:\n",
    "            gray_mask = np.mean(new_segmented_image, axis=2)\n",
    "        else:\n",
    "            gray_mask = new_segmented_image\n",
    "        \n",
    "        # Resize to match volume dimensions and store\n",
    "        from skimage.transform import resize\n",
    "        processed_volume[:, :, slice_idx] = resize(gray_mask, (vol_data.shape[0], vol_data.shape[1]), preserve_range=True)\n",
    "    \n",
    "    return processed_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcfe588",
   "metadata": {},
   "source": [
    "## Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dd45f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 160, 160)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = '/home/fetalusr1/Fetal-Head-Segmentation-master/28GW.nii'\n",
    "text_prompt = ['fetal head']\n",
    "vol = nib.load(image_path)\n",
    "vol_data = vol.get_fdata()\n",
    "vol_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "214efc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting GPU Inference on 160 slices...\n",
      "   (Using Custom Reader + Autocast)\n",
      "   ‚úÖ Processed slice 0/160\n",
      "   ‚úÖ Processed slice 10/160\n",
      "   ‚úÖ Processed slice 20/160\n",
      "   ‚úÖ Processed slice 30/160\n",
      "   ‚úÖ Processed slice 40/160\n",
      "   ‚úÖ Processed slice 50/160\n",
      "   ‚úÖ Processed slice 60/160\n",
      "   ‚úÖ Processed slice 70/160\n",
      "   ‚úÖ Processed slice 80/160\n",
      "   ‚úÖ Processed slice 90/160\n",
      "   ‚úÖ Processed slice 100/160\n",
      "   ‚úÖ Processed slice 110/160\n",
      "   ‚úÖ Processed slice 120/160\n",
      "   ‚úÖ Processed slice 130/160\n",
      "   ‚úÖ Processed slice 140/160\n",
      "   ‚úÖ Processed slice 150/160\n",
      "üéâ Inference Complete! Starting post-processing...\n",
      "Processing volume with shape: (160, 160, 160)\n",
      "Reference slice: 80\n",
      "Found 1496 regions across 160 slices\n",
      "Reference measurements - Major: 0.00, Minor: 0.00\n",
      "Filtering thresholds - Major: 0.00, Minor: 0.00\n",
      "After filtering: 1496 regions in 160 slices\n",
      "Original volume had 1798198 non-zero voxels\n",
      "Processed volume has 1798198 non-zero voxels\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "import torch\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "\n",
    "# --- 1. DEFINE A ROBUST CUSTOM READER ---\n",
    "def custom_read_nifti(file_path, slice_idx):\n",
    "    \"\"\"\n",
    "    Safely reads a slice from a NIfTI file, handling 3D/4D shapes automatically.\n",
    "    Replaces the buggy read_nifti from the library.\n",
    "    \"\"\"\n",
    "    # Load volume\n",
    "    vol = nib.load(file_path).get_fdata()\n",
    "    \n",
    "    # Handle dimensions (3D vs 4D)\n",
    "    if vol.ndim == 4:\n",
    "        # If 4D, take the first channel (standard for medical data)\n",
    "        slice_data = vol[:, :, slice_idx, 0]\n",
    "    else:\n",
    "        # If 3D, just take the slice\n",
    "        slice_data = vol[:, :, slice_idx]\n",
    "\n",
    "    # Normalize Intensity (Robust Percentile Scaling)\n",
    "    # This prevents black/white outliers from ruining the image\n",
    "    p_low = np.percentile(slice_data, 0.5)\n",
    "    p_high = np.percentile(slice_data, 99.5)\n",
    "    slice_data = np.clip(slice_data, p_low, p_high)\n",
    "    \n",
    "    # Scale to 0-255 (uint8) for the model\n",
    "    if slice_data.max() > slice_data.min():\n",
    "        slice_data = (slice_data - slice_data.min()) / (slice_data.max() - slice_data.min())\n",
    "    slice_data = (slice_data * 255).astype(np.uint8)\n",
    "    \n",
    "    # Convert grayscale to RGB (H, W, 3) because the model expects color input\n",
    "    image_rgb = np.stack([slice_data, slice_data, slice_data], axis=-1)\n",
    "    \n",
    "    return image_rgb\n",
    "\n",
    "# --- 2. RUN INFERENCE USING THE CUSTOM READER ---\n",
    "\n",
    "# Initialize volume\n",
    "vol_data = nib.load(image_path).get_fdata() # Just to get the shape\n",
    "pred_volume = np.zeros((vol_data.shape[0], vol_data.shape[1], vol_data.shape[2]))\n",
    "\n",
    "print(f\"üöÄ Starting GPU Inference on {vol_data.shape[2]} slices...\")\n",
    "print(f\"   (Using Custom Reader + Autocast)\")\n",
    "\n",
    "for slice_idx in range(vol_data.shape[2]):\n",
    "    \n",
    "    # Clean GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # A. Use our NEW reader (Bypassing the library error)\n",
    "    image = custom_read_nifti(image_path, slice_idx)\n",
    "    \n",
    "    # B. Run Model (With mixed precision to fit in memory)\n",
    "    with autocast():\n",
    "        # Note: We call interactive_infer_image directly since we already read the image\n",
    "        pred_mask, out_prob = interactive_infer_image(model, Image.fromarray(image), text_prompt)\n",
    "    \n",
    "    # C. Process Mask\n",
    "    # The model output is often [1, H, W], we need [H, W]\n",
    "    if len(pred_mask.shape) == 3:\n",
    "        mask_2d = pred_mask[0]\n",
    "    else:\n",
    "        mask_2d = pred_mask\n",
    "\n",
    "    # D. Save to volume\n",
    "    pred_volume[:, :, slice_idx] = resize(mask_2d, (vol_data.shape[0], vol_data.shape[1]), preserve_range=True)\n",
    "\n",
    "    # Progress Print\n",
    "    if slice_idx % 10 == 0:\n",
    "        print(f\"   ‚úÖ Processed slice {slice_idx}/{vol_data.shape[2]}\")\n",
    "\n",
    "print(\"üéâ Inference Complete! Starting post-processing...\")\n",
    "\n",
    "# --- 3. RUN POST-PROCESSING ---\n",
    "processed_volume, filtered_measurements = process_predicted_volume(\n",
    "    pred_volume, \n",
    "    threshold_factor=0.4, \n",
    "    output_prefix='3_2'\n",
    ")\n",
    "\n",
    "print(f\"Original volume had {np.sum(pred_volume > 0)} non-zero voxels\")\n",
    "print(f\"Processed volume has {np.sum(processed_volume > 0)} non-zero voxels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b52612c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First filtered slice: 0\n",
      "Last filtered slice: 159\n",
      "Blank slices from 0 to 159: []\n"
     ]
    }
   ],
   "source": [
    "#Get the first slice that survived filtering\n",
    "first_filtered_slice = min(filtered_measurements['slice_index'].unique())\n",
    "last_filtered_slice = max(filtered_measurements['slice_index'].unique())\n",
    "print(f\"First filtered slice: {first_filtered_slice}\")\n",
    "print(f\"Last filtered slice: {last_filtered_slice}\")\n",
    "#from the filtered slice to the center slice, get all the slices which are blank\n",
    "blank_slices = []\n",
    "for slice_idx in range(first_filtered_slice, last_filtered_slice + 1):\n",
    "    if np.sum(processed_volume[:, :, slice_idx]) == 0:\n",
    "        blank_slices.append(slice_idx)\n",
    "# Print the blank slices\n",
    "print(f\"Blank slices from {first_filtered_slice} to {vol_data.shape[2]-1}: {blank_slices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b805daa1",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f99be588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw prediction saved to ./results/segmentation_result_3_2_raw.nii.gz\n",
      "Processed prediction saved to ./FilteredRes/segmentation_result_3_2_filtered.nii.gz\n",
      "Interpolated prediction saved to ./FilteredRes/segmentation_result_3_2_interpolated.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create results directories if they don't exist\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "os.makedirs('./FilteredRes', exist_ok=True)\n",
    "\n",
    "# Load original NIfTI for header info\n",
    "original_nii = nib.load(image_path)\n",
    "\n",
    "# Save raw prediction\n",
    "pred_nii = nib.Nifti1Image(pred_volume, original_nii.affine, original_nii.header)\n",
    "raw_filename = f'./results/segmentation_result_3_2_raw.nii.gz'\n",
    "nib.save(pred_nii, raw_filename)\n",
    "print(f\"Raw prediction saved to {raw_filename}\")\n",
    "\n",
    "# Save processed prediction\n",
    "processed_nii = nib.Nifti1Image(processed_volume, original_nii.affine, original_nii.header)\n",
    "processed_filename = f'./FilteredRes/segmentation_result_3_2_filtered.nii.gz'\n",
    "nib.save(processed_nii, processed_filename)\n",
    "print(f\"Processed prediction saved to {processed_filename}\")\n",
    "\n",
    "interpolated_volume = interpolate_blank_slices(image_path, processed_volume, blank_slices, predicted_masks, delta=1)\n",
    "# Save interpolated prediction\n",
    "interpolated_nii = nib.Nifti1Image(interpolated_volume, original_nii.affine, original_nii.header)\n",
    "interpolated_filename = f'./FilteredRes/segmentation_result_3_2_interpolated.nii.gz'\n",
    "nib.save(interpolated_nii, interpolated_filename)\n",
    "print(f\"Interpolated prediction saved to {interpolated_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf2f648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fetal_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
