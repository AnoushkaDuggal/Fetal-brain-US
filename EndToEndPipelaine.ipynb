{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a16ab453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Ready to process: /home/fetalusr1/Fetal-Head-Segmentation-master/IMG_20250329_13_1.nii\n",
      "ðŸ’¾ Results will save to: ./FilteredRes/segmentation_result_13_1_interpolated.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# GLOBAL CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "# Change this path whenever you have a new sample\n",
    "FILE_ID = \"13_1\"  \n",
    "IMAGE_PATH = f\"/home/fetalusr1/Fetal-Head-Segmentation-master/IMG_20250329_13_1.nii\"\n",
    "\n",
    "# This automatically names your outputs based on the input\n",
    "MASK_OUTPUT_PATH = f\"./FilteredRes/segmentation_result_{FILE_ID}_interpolated.nii.gz\"\n",
    "REPORT_ZIP_NAME = f\"3D_Full_Report_{FILE_ID}.zip\"\n",
    "\n",
    "print(f\"ðŸš€ Ready to process: {IMAGE_PATH}\")\n",
    "print(f\"ðŸ’¾ Results will save to: {MASK_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d97dfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Memory fragmentation rules applied.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# optimizing memory allocation to reduce fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "print(\"âœ… Memory fragmentation rules applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2db9a473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Memory settings applied. Free memory: 43.29 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# 1. Help PyTorch manage fragmented memory\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# 2. Clear any lingering cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"âœ… Memory settings applied. Free memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f6bfc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Forced Library Path: /home/fetalusr1/miniconda3/envs/fetal_project/lib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the path to your current environment\n",
    "conda_prefix = sys.prefix\n",
    "lib_path = os.path.join(conda_prefix, 'lib')\n",
    "\n",
    "# Force this path to the front of the line\n",
    "os.environ['LD_LIBRARY_PATH'] = f\"{lib_path}:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "\n",
    "print(f\"âœ… Forced Library Path: {lib_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c686b2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from modeling.BaseModel import BaseModel\n",
    "from modeling import build_model\n",
    "from utilities.distributed import init_distributed\n",
    "from utilities.arguments import load_opt_from_config_files\n",
    "from utilities.constants import BIOMED_CLASSES\n",
    "import matplotlib.pyplot as plt\n",
    "from inference_utils.inference import interactive_infer_image\n",
    "from inference_utils.output_processing import check_mask_stats\n",
    "from inference_utils.processing_utils import process_intensity_image\n",
    "from inference_utils.processing_utils import read_nifti\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import SimpleITK as sitk\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "out_probs = []\n",
    "predicted_masks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9f2c29",
   "metadata": {},
   "source": [
    "## Loading the Finetuned BiomedParse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b84965ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$UNUSED$ criterion.empty_weight, Ckpt Shape: torch.Size([17])\n"
     ]
    }
   ],
   "source": [
    "# Build model config\n",
    "opt = load_opt_from_config_files([\"configs/biomedparse_inference.yaml\"])\n",
    "opt = init_distributed(opt)\n",
    "\n",
    "# Load model from pretrained weights\n",
    "finetuned_pth = '/home/fetalusr1/Fetal-Head-Segmentation-master/model_state_dict.pt' # Replace with the path to your finetuned checkpoint\n",
    "\n",
    "model = BaseModel(opt, build_model(opt)).from_pretrained(pretrained=finetuned_pth).eval().cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.model.sem_seg_head.predictor.lang_encoder.get_text_embeddings(BIOMED_CLASSES + [\"background\"], is_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc46355",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7089a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation_masks(original_image, segmentation_masks, texts, rotate=0):\n",
    "    ''' Plot a list of segmentation mask over an image showing only the segmented region.\n",
    "    '''\n",
    "    original_image = original_image[:, :, :3]\n",
    "\n",
    "    segmented_images = []\n",
    "\n",
    "    for i, mask in enumerate(segmentation_masks):\n",
    "        segmented_image = original_image.copy()\n",
    "        segmented_image[mask <= 0.5] = [0, 0, 0]\n",
    "        segmented_images.append(segmented_image)\n",
    "        \n",
    "    return segmented_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d04f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_nifti(file_path, text_prompts, is_CT, slice_idx, site=None, HW_index=(0, 1), channel_idx=None, rotate=0):\n",
    "\n",
    "    #image = read_nifti(file_path, is_CT, slice_idx, site=site, HW_index=HW_index, channel_idx=channel_idx)\n",
    "    image, pad_width, prepad_shape = read_nifti(file_path, is_CT, slice_idx, site=site, HW_index=HW_index, channel_idx=channel_idx)\n",
    "\n",
    "    pred_mask,out_prob = interactive_infer_image(model, Image.fromarray(image), text_prompts)\n",
    "    predicted_masks.append(pred_mask)\n",
    "    segmented_images = get_segmentation_masks(image, pred_mask, text_prompts, rotate=rotate)\n",
    "    out_probs.append(out_prob)\n",
    "    \n",
    "    return image, pred_mask, segmented_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7571fe0b",
   "metadata": {},
   "source": [
    "### Post-processing Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3d7a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predicted_volume(volume_data, threshold_factor=0.35, output_prefix='processed'):\n",
    "    \"\"\"\n",
    "    Process the predicted volume to filter based on ellipse measurements.\n",
    "    \"\"\"\n",
    "    data = volume_data\n",
    "    print(f\"Processing volume with shape: {data.shape}\")\n",
    "    \n",
    "    # Calculate measurements for all slices\n",
    "    results = []\n",
    "    z_0 = data.shape[2] // 2  # Reference slice (middle slice)\n",
    "    \n",
    "    print(f\"Reference slice: {z_0}\")\n",
    "    \n",
    "    for i in range(data.shape[2]):\n",
    "        slice_data = data[:, :, i]\n",
    "        \n",
    "        # Skip empty slices\n",
    "        if np.sum(slice_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Binarize the slice\n",
    "        slice_bin = np.where(slice_data > 0, 1, 0).astype(np.uint8)\n",
    "        \n",
    "        # Fill holes\n",
    "        slice_bin_filled = sitk.BinaryFillhole(sitk.GetImageFromArray(slice_bin))\n",
    "        slice_bin_filled = sitk.GetArrayFromImage(slice_bin_filled)\n",
    "        \n",
    "        # Get region properties\n",
    "        labeled_image = label(slice_bin_filled)\n",
    "        props = regionprops(labeled_image)\n",
    "        \n",
    "        for prop in props:\n",
    "            results.append({\n",
    "                'slice_index': i,\n",
    "                'major_axis_length': prop.major_axis_length,\n",
    "                'minor_axis_length': prop.minor_axis_length,\n",
    "                'centroid_x': prop.centroid[1],\n",
    "                'centroid_y': prop.centroid[0],\n",
    "                'orientation': prop.orientation,\n",
    "                'area': prop.area\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(f\"Found {len(results)} regions across {len(df_results['slice_index'].unique())} slices\")\n",
    "    \n",
    "    # Get reference slice measurements for filtering\n",
    "    standard_slice_data = df_results[df_results['slice_index'] == z_0]\n",
    "    \n",
    "    if standard_slice_data.empty:\n",
    "        print(f\"Warning: No data found in reference slice {z_0}\")\n",
    "        # Use overall median as fallback\n",
    "        major_axis_length_std = df_results['major_axis_length'].median()\n",
    "        minor_axis_length_std = df_results['minor_axis_length'].median()\n",
    "        centroid_x_std = df_results['centroid_x'].median()\n",
    "        centroid_y_std = df_results['centroid_y'].median()\n",
    "    else:\n",
    "        major_axis_length_std = standard_slice_data['major_axis_length'].values[0]\n",
    "        minor_axis_length_std = standard_slice_data['minor_axis_length'].values[0]\n",
    "        centroid_x_std = standard_slice_data['centroid_x'].values[0]\n",
    "        centroid_y_std = standard_slice_data['centroid_y'].values[0]\n",
    "    \n",
    "    # Define thresholds\n",
    "    major_axis_length_threshold = major_axis_length_std * (1 - threshold_factor)\n",
    "    minor_axis_length_threshold = minor_axis_length_std * (1 - threshold_factor)\n",
    "    \n",
    "    print(f\"Reference measurements - Major: {major_axis_length_std:.2f}, Minor: {minor_axis_length_std:.2f}\")\n",
    "    print(f\"Filtering thresholds - Major: {major_axis_length_threshold:.2f}, Minor: {minor_axis_length_threshold:.2f}\")\n",
    "    \n",
    "    # Filter based on thresholds\n",
    "    filtered_df = df_results[\n",
    "        (df_results['major_axis_length'] >= major_axis_length_threshold) &\n",
    "        (df_results['minor_axis_length'] >= minor_axis_length_threshold)\n",
    "    ]\n",
    "    \n",
    "    print(f\"After filtering: {len(filtered_df)} regions in {len(filtered_df['slice_index'].unique())} slices\")\n",
    "    \n",
    "    # In filtered_df, in case of repeated slices, keep the one with maximum major axis length\n",
    "    filtered_df = filtered_df.loc[filtered_df.groupby('slice_index')['major_axis_length'].idxmax()]\n",
    "    \n",
    "    # Create filtered volume\n",
    "    filtered_slices = filtered_df['slice_index'].unique()\n",
    "    filtered_volume = np.zeros_like(data)\n",
    "    \n",
    "    for slice_idx in range(data.shape[2]):\n",
    "        if slice_idx in filtered_slices:\n",
    "            filtered_volume[:, :, slice_idx] = data[:, :, slice_idx]\n",
    "    \n",
    "    return filtered_volume, filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46da5a2",
   "metadata": {},
   "source": [
    "### Interpolation Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "164a72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_blank_slices(image_path, processed_volume, blank_slices, predicted_masks, delta=1):\n",
    "    \"\"\"\n",
    "    Interpolate blank slices in the processed volume using the previous slice.\n",
    "    \"\"\"\n",
    "    vol_data = nib.load(image_path).get_fdata()\n",
    "    central_slice = vol_data.shape[2] // 2\n",
    "    \n",
    "    for slice_idx in blank_slices:\n",
    "        # Ensure we have a valid previous slice\n",
    "        prev_slice_idx = slice_idx - delta\n",
    "        if prev_slice_idx < 0 or prev_slice_idx >= len(predicted_masks):\n",
    "            continue\n",
    "            \n",
    "        # Get the previous mask\n",
    "        prev_mask = predicted_masks[prev_slice_idx][0]  # Get first mask from the list\n",
    "        \n",
    "        #update predicted_masks\n",
    "        predicted_masks[slice_idx] = [prev_mask.copy()]  # Store the previous mask\n",
    "        # Ensure the previous mask is not empty\n",
    "        if np.sum(prev_mask) == 0:\n",
    "            print(f\"Warning: Previous mask for slice {prev_slice_idx} is empty. Skipping interpolation for slice {slice_idx}.\")\n",
    "            continue\n",
    "        # Scale the mask based on position relative to center\n",
    "        if slice_idx < central_slice: \n",
    "            # Increase the mask size by 0.5%\n",
    "            new_mask = prev_mask * 1.005\n",
    "        else:\n",
    "            # Decrease the mask size by 0.5%\n",
    "            new_mask = prev_mask * 0.995\n",
    "        \n",
    "        # Read the original image for this slice\n",
    "        image = read_nifti(image_path, is_CT=False, slice_idx=slice_idx, site=None, HW_index=(0, 1), channel_idx=None)\n",
    "        \n",
    "        # Get the segmented image\n",
    "        new_segmented_image = get_segmentation_masks(image, [new_mask], ['fetal head'], rotate=0)[0]\n",
    "        \n",
    "        # Convert RGB segmentation to grayscale if needed\n",
    "        if len(new_segmented_image.shape) == 3:\n",
    "            gray_mask = np.mean(new_segmented_image, axis=2)\n",
    "        else:\n",
    "            gray_mask = new_segmented_image\n",
    "        \n",
    "        # Resize to match volume dimensions and store\n",
    "        from skimage.transform import resize\n",
    "        processed_volume[:, :, slice_idx] = resize(gray_mask, (vol_data.shape[0], vol_data.shape[1]), preserve_range=True)\n",
    "    \n",
    "    return processed_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcfe588",
   "metadata": {},
   "source": [
    "## Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d578d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227, 149, 234)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = '/home/fetalusr1/Fetal-Head-Segmentation-master/IMG_20250329_13_1.nii'\n",
    "text_prompt = ['fetal head']\n",
    "vol = nib.load(image_path)\n",
    "vol_data = vol.get_fdata()\n",
    "vol_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b999746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import transform\n",
    "\n",
    "def process_intensity_image_with_metadata(image_data, is_CT, site=None):\n",
    "    # 1. Normalization (Existing logic)\n",
    "    if is_CT:\n",
    "        image_data[image_data < -200] = -200\n",
    "        image_data[image_data > 200] = 200\n",
    "        image_data = (image_data + 200) / 400.0\n",
    "    else:\n",
    "        # MRI/Ultrasound normalization\n",
    "        max_val = np.percentile(image_data, 99)\n",
    "        if max_val > 0:\n",
    "            image_data = image_data / max_val\n",
    "        image_data = np.clip(image_data, 0, 1)\n",
    "\n",
    "    # 2. Capture Original Shape\n",
    "    h_orig, w_orig = image_data.shape[:2]\n",
    "\n",
    "    # 3. Calculate Padding (The \"Receipt\")\n",
    "    # We want to fit into 1024x1024 while keeping aspect ratio\n",
    "    target_size = 1024\n",
    "    scale = target_size / max(h_orig, w_orig)\n",
    "    \n",
    "    h_new = int(h_orig * scale)\n",
    "    w_new = int(w_orig * scale)\n",
    "    \n",
    "    image_resized = transform.resize(image_data, (h_new, w_new), preserve_range=True)\n",
    "\n",
    "    # Calculate how much padding is needed to reach 1024\n",
    "    pad_h = target_size - h_new\n",
    "    pad_w = target_size - w_new\n",
    "    \n",
    "    # We pad symmetrically (half on top/left, half on bottom/right)\n",
    "    pad_top = pad_h // 2\n",
    "    pad_bottom = pad_h - pad_top\n",
    "    pad_left = pad_w // 2\n",
    "    pad_right = pad_w - pad_left\n",
    "    \n",
    "    # Apply Padding\n",
    "    # ((top, bottom), (left, right))\n",
    "    padding_coords = ((pad_top, pad_bottom), (pad_left, pad_right))\n",
    "    image_padded = np.pad(image_resized, padding_coords, mode='constant', constant_values=0)\n",
    "    \n",
    "    # Stack to 3 channels (Model expectation)\n",
    "    image_final = np.stack([image_padded]*3, axis=-1)\n",
    "    \n",
    "\n",
    "    metadata = {\n",
    "        'orig_shape': (h_orig, w_orig),\n",
    "        'scale': scale,\n",
    "        'padding': padding_coords,  # ((top, bottom), (left, right))\n",
    "        'final_shape': (target_size, target_size)\n",
    "    }\n",
    "    \n",
    "    return image_final, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ef2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "def read_nifti_with_metadata(file_path, is_CT, slice_idx, site=None, HW_index=(0, 1), channel_idx=None):\n",
    "    # Load Volume\n",
    "    nii = nib.load(file_path)\n",
    "    data = nii.get_fdata()\n",
    "\n",
    "    # Handle Dimensions\n",
    "    if channel_idx is not None:\n",
    "        data = np.take(data, indices=0, axis=channel_idx)\n",
    "    \n",
    "    # Extract Slice\n",
    "    # Assuming standard orientation, modify if your specific slicing differs\n",
    "    if data.ndim == 3:\n",
    "        image_slice = data[:, :, slice_idx]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected volume dimension: {data.ndim}\")\n",
    "        \n",
    "    # Rotate if necessary (Standardize orientation)\n",
    "    # image_slice = np.rot90(image_slice, k=1) # Uncomment if you need rotation\n",
    "\n",
    "    # CALL THE NEW PROCESSOR\n",
    "    image_processed, metadata = process_intensity_image_with_metadata(image_slice, is_CT, site)\n",
    "    \n",
    "    return image_processed, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7493bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def inference_nifti_geometry_aware(file_path, text_prompts, slice_idx):\n",
    "    # 1. READ WITH METADATA\n",
    "    # Ensure read_nifti_with_metadata is defined (from previous steps)\n",
    "    image_input, metadata = read_nifti_with_metadata(\n",
    "        file_path, is_CT=False, slice_idx=slice_idx\n",
    "    )\n",
    "    \n",
    "    # 2. INFERENCE\n",
    "\n",
    "    pred_mask, _ = interactive_infer_image(model, Image.fromarray((image_input*255).astype(np.uint8)), text_prompts)\n",
    "    \n",
    "    # Handle Tensor vs Numpy\n",
    "    if isinstance(pred_mask, torch.Tensor):\n",
    "        pred_mask = pred_mask.detach().cpu().numpy()\n",
    "        \n",
    "    # pred_mask is 1024x1024. Squeeze to remove batch dims if any.\n",
    "    pred_mask = pred_mask.squeeze()\n",
    "    \n",
    "    # 3. THE UN-SQUISHER (Padding Inversion)\n",
    "    pad_top, pad_bottom = metadata['padding'][0]\n",
    "    pad_left, pad_right = metadata['padding'][1]\n",
    "    \n",
    "    h_padded, w_padded = metadata['final_shape']\n",
    "    \n",
    "    # Calculate valid region (where the actual image sits inside the black bars)\n",
    "    valid_h_start = pad_top\n",
    "    valid_h_end = h_padded - pad_bottom\n",
    "    valid_w_start = pad_left\n",
    "    valid_w_end = w_padded - pad_right\n",
    "    \n",
    "    # Crop out the padding\n",
    "    mask_cropped = pred_mask[valid_h_start:valid_h_end, valid_w_start:valid_w_end]\n",
    "    \n",
    "    # 4. RESIZE TO NATIVE\n",
    "    # Now that padding is gone, we resize to the original anatomical shape\n",
    "    orig_h, orig_w = metadata['orig_shape']\n",
    "    \n",
    "    # Use order=0 (nearest neighbor) to keep the mask sharp/binary\n",
    "    mask_native = resize(mask_cropped, (orig_h, orig_w), order=0, preserve_range=True, anti_aliasing=False)\n",
    "    \n",
    "    # Return as boolean/binary\n",
    "    return mask_native > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "958dca41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Geometry-Aware Inference...\n",
      "âœ… Saved native-geometry segmentation to segmentation_native_fixed.nii.gz\n",
      "Processing volume with shape: (227, 149, 234)\n",
      "Reference slice: 117\n",
      "Found 273 regions across 234 slices\n",
      "Reference measurements - Major: 166.54, Minor: 136.07\n",
      "Filtering thresholds - Major: 99.92, Minor: 81.64\n",
      "After filtering: 110 regions in 110 slices\n",
      "Original volume had 1967965 non-zero voxels\n",
      "Processed volume has 1625277 non-zero voxels\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. SETUP VOLUME\n",
    "pred_volume = np.zeros(\n",
    "    (vol_data.shape[0], vol_data.shape[1], vol_data.shape[2]),\n",
    "    dtype=np.uint8\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Starting Geometry-Aware Inference...\")\n",
    "\n",
    "# 2. THE LOOP\n",
    "for slice_idx in range(vol_data.shape[2]):\n",
    "    \n",
    "    # CHANGE 1: Use the new function. \n",
    "    # It returns the mask ALREADY in native shape (e.g., 277x149).\n",
    "    # We do not need 'image' or '_' here.\n",
    "    native_mask = inference_nifti_geometry_aware(\n",
    "        image_path, \n",
    "        text_prompt, \n",
    "        slice_idx=slice_idx\n",
    "    )\n",
    "\n",
    "    # CHANGE 2: No unpacking needed, no resizing needed.\n",
    "    # The function already cropped the padding and resized strictly valid data.\n",
    "    \n",
    "    # Just threshold and assign\n",
    "    # (The function returns a boolean mask, convert to uint8)\n",
    "    pred_volume[:, :, slice_idx] = native_mask.astype(np.uint8)\n",
    "\n",
    "# 3. SAVE (Exact same logic as you had, which is correct)\n",
    "original_nii = nib.load(image_path)\n",
    "\n",
    "seg_nii = nib.Nifti1Image(\n",
    "    pred_volume,\n",
    "    original_nii.affine,\n",
    "    original_nii.header\n",
    ")\n",
    "\n",
    "nib.save(seg_nii, \"./results/segmentationd_native_fixed.nii.gz\")\n",
    "print(\"âœ… Saved native-geometry segmentation to segmentation_native_fixed.nii.gz\")\n",
    "\n",
    "# 4. POST-PROCESSING (Unchanged)\n",
    "processed_volume, filtered_measurements = process_predicted_volume(\n",
    "    pred_volume, \n",
    "    threshold_factor=0.4,\n",
    "    output_prefix='3_2'\n",
    ")\n",
    "\n",
    "print(f\"Original volume had {np.sum(pred_volume > 0)} non-zero voxels\")\n",
    "print(f\"Processed volume has {np.sum(processed_volume > 0)} non-zero voxels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c90c314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "from skimage.measure import label, regionprops\n",
    "import pandas as pd\n",
    "import scipy.ndimage as ndimage\n",
    "import numpy as np\n",
    "\n",
    "def process_predicted_volume(volume_data, threshold_factor=0.35):\n",
    "    \"\"\"\n",
    "    Filters out 'bad' slices where the prediction is too small or weirdly shaped\n",
    "    compared to the center slice.\n",
    "    \"\"\"\n",
    "    data = volume_data.copy()\n",
    "    results = []\n",
    "    z_0 = data.shape[2] // 2  # Center slice\n",
    "    \n",
    "    # 1. Analyze every slice\n",
    "    for i in range(data.shape[2]):\n",
    "        slice_data = data[:, :, i]\n",
    "        if np.sum(slice_data) == 0: continue\n",
    "\n",
    "        # SimpleITK hole filling (Clean up small black spots inside the head)\n",
    "        slice_sitk = sitk.GetImageFromArray(slice_data.astype(np.uint8))\n",
    "        slice_filled = sitk.BinaryFillhole(slice_sitk)\n",
    "        slice_data = sitk.GetArrayFromImage(slice_filled)\n",
    "\n",
    "        # Measure properties\n",
    "        labeled_img = label(slice_data)\n",
    "        props = regionprops(labeled_img)\n",
    "        \n",
    "        # Take the largest object if multiple exist\n",
    "        if props:\n",
    "            main_blob = max(props, key=lambda x: x.area)\n",
    "            results.append({\n",
    "                'slice_index': i,\n",
    "                'major_axis': main_blob.major_axis_length,\n",
    "                'minor_axis': main_blob.minor_axis_length,\n",
    "                'area': main_blob.area\n",
    "            })\n",
    "    \n",
    "    # 2. Filter Outliers\n",
    "    df = pd.DataFrame(results)\n",
    "    if df.empty: return np.zeros_like(data), df\n",
    "\n",
    "    # Get baseline from center slice\n",
    "    center_stats = df[df['slice_index'] == z_0]\n",
    "    if center_stats.empty:\n",
    "        # Fallback to median if center is empty\n",
    "        baseline_major = df['major_axis'].median()\n",
    "        baseline_minor = df['minor_axis'].median()\n",
    "    else:\n",
    "        baseline_major = center_stats['major_axis'].values[0]\n",
    "        baseline_minor = center_stats['minor_axis'].values[0]\n",
    "\n",
    "    # Thresholds (e.g., must be at least 65% of the size of the center)\n",
    "    thresh_major = baseline_major * (1 - threshold_factor)\n",
    "    thresh_minor = baseline_minor * (1 - threshold_factor)\n",
    "\n",
    "    valid_slices = df[\n",
    "        (df['major_axis'] >= thresh_major) & \n",
    "        (df['minor_axis'] >= thresh_minor)\n",
    "    ]['slice_index'].unique()\n",
    "\n",
    "    # 3. Create Clean Volume\n",
    "    filtered_vol = np.zeros_like(data)\n",
    "    for idx in valid_slices:\n",
    "        filtered_vol[:, :, idx] = data[:, :, idx]\n",
    "        \n",
    "    return filtered_vol, valid_slices\n",
    "\n",
    "def interpolate_gaps(volume, valid_slices):\n",
    "    \"\"\"\n",
    "    Fills in the blanks between valid slices using geometric dilation/erosion.\n",
    "    \"\"\"\n",
    "    filled_vol = volume.copy()\n",
    "    min_slice = min(valid_slices)\n",
    "    max_slice = max(valid_slices)\n",
    "    center_slice = volume.shape[2] // 2\n",
    "    \n",
    "    # We iterate through the whole range of the head\n",
    "    for i in range(min_slice, max_slice + 1):\n",
    "        if i in valid_slices:\n",
    "            continue # Skip if we already have data\n",
    "            \n",
    "        # If blank, look at neighbors\n",
    "        # We prefer the 'previous' slice if we are before the center\n",
    "        # We prefer the 'next' slice if we are after the center\n",
    "        if i < center_slice:\n",
    "            ref_idx = i - 1\n",
    "            if ref_idx in valid_slices:\n",
    "                ref_mask = filled_vol[:, :, ref_idx]\n",
    "                # Moving toward center -> Head gets bigger -> Dilate\n",
    "                new_mask = ndimage.binary_dilation(ref_mask, iterations=1)\n",
    "                filled_vol[:, :, i] = new_mask\n",
    "        else:\n",
    "            ref_idx = i - 1 # Simple forward fill for now\n",
    "            if ref_idx in valid_slices:\n",
    "                ref_mask = filled_vol[:, :, ref_idx]\n",
    "                # Moving away from center -> Head gets smaller -> Erode\n",
    "                new_mask = ndimage.binary_erosion(ref_mask, iterations=1)\n",
    "                filled_vol[:, :, i] = new_mask\n",
    "                \n",
    "    return filled_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "202fae94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ 1. Running Geometry-Aware Inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fetalusr1/miniconda3/envs/fetal_project/lib/python3.8/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  max_size = (max_size + (stride - 1)) // stride * stride\n",
      "/home/fetalusr1/Fetal-Head-Segmentation-master/modeling/modules/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
      "/home/fetalusr1/miniconda3/envs/fetal_project/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ 2. Filtering Bad Slices...\n",
      "   - Kept 110 slices. Dropped 124.\n",
      "ðŸ§± 3. Interpolating Gaps...\n",
      "ðŸ’¾ 4. Saving Final Result...\n",
      "âœ… DONE! Saved to: ./results/segmentation_FINAL_FIXED.nii.gz\n",
      "   - Geometry: CORRECT (Native)\n",
      "   - Outliers: REMOVED\n",
      "   - Gaps:     FILLED\n"
     ]
    }
   ],
   "source": [
    "# 1. SETUP\n",
    "pred_volume = np.zeros(\n",
    "    (vol_data.shape[0], vol_data.shape[1], vol_data.shape[2]), \n",
    "    dtype=np.uint8\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ 1. Running Geometry-Aware Inference...\")\n",
    "for slice_idx in range(vol_data.shape[2]):\n",
    "    # Use our FIXED inference function\n",
    "    native_mask = inference_nifti_geometry_aware(\n",
    "        image_path, text_prompt, slice_idx=slice_idx\n",
    "    )\n",
    "    pred_volume[:, :, slice_idx] = native_mask.astype(np.uint8)\n",
    "\n",
    "print(\"ðŸ§¹ 2. Filtering Bad Slices...\")\n",
    "filtered_volume, valid_slices = process_predicted_volume(pred_volume, threshold_factor=0.4)\n",
    "print(f\"   - Kept {len(valid_slices)} slices. Dropped {pred_volume.shape[2] - len(valid_slices)}.\")\n",
    "\n",
    "print(\"ðŸ§± 3. Interpolating Gaps...\")\n",
    "final_volume = interpolate_gaps(filtered_volume, valid_slices)\n",
    "\n",
    "# 4. SAVE\n",
    "print(\"ðŸ’¾ 4. Saving Final Result...\")\n",
    "original_nii = nib.load(image_path)\n",
    "\n",
    "# Save the completely fixed volume\n",
    "final_nii = nib.Nifti1Image(\n",
    "    final_volume.astype(np.float32), \n",
    "    original_nii.affine, \n",
    "    original_nii.header\n",
    ")\n",
    "\n",
    "save_path = \"./results/segmentation_FINAL_FIXED.nii.gz\"\n",
    "nib.save(final_nii, save_path)\n",
    "\n",
    "print(f\"âœ… DONE! Saved to: {save_path}\")\n",
    "print(\"   - Geometry: CORRECT (Native)\")\n",
    "print(\"   - Outliers: REMOVED\")\n",
    "print(\"   - Gaps:     FILLED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02a75afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¨ Creating Final Masked Ultrasound...\n",
      "âœ… Success! Saved to: ./results/masked_ultrasound_FINAL_FIXED.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "# 1. SETUP PATHS\n",
    "# The original raw ultrasound (The Texture)\n",
    "orig_path = image_path \n",
    "\n",
    "# The final cleaned mask you just generated (The Shape)\n",
    "mask_path = \"./results/segmentation_FINAL_FIXED.nii.gz\"\n",
    "\n",
    "# The output filename\n",
    "output_path = \"./results/masked_ultrasound_FINAL_FIXED.nii.gz\"\n",
    "\n",
    "print(f\"ðŸ”¨ Creating Final Masked Ultrasound...\")\n",
    "\n",
    "# 2. LOAD\n",
    "orig_nii = nib.load(orig_path)\n",
    "mask_nii = nib.load(mask_path)\n",
    "\n",
    "orig_data = orig_nii.get_fdata()\n",
    "mask_data = mask_nii.get_fdata()\n",
    "\n",
    "# 3. APPLY MASK (The \"Cookie Cutter\")\n",
    "# Formula: Ultrasound Pixel * Mask (0 or 1)\n",
    "# Result: Brain texture inside, Black void outside.\n",
    "masked_volume = orig_data * (mask_data > 0).astype(orig_data.dtype)\n",
    "\n",
    "# 4. SAVE\n",
    "final_nii = nib.Nifti1Image(\n",
    "    masked_volume, \n",
    "    orig_nii.affine, \n",
    "    orig_nii.header\n",
    ")\n",
    "\n",
    "nib.save(final_nii, output_path)\n",
    "\n",
    "print(f\"âœ… Success! Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15099f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¨ Creating Masked Ultrasound...\n",
      "âœ… Success! Saved to: ./results/masked_ultrasound_native.nii.gz\n",
      "   Open this file in FreeView. You should see ONLY the fetal head.\n",
      "   The background will be completely black.\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. SETUP PATHS\n",
    "# The original raw ultrasound\n",
    "original_nii_path = image_path \n",
    "# The geometry-corrected binary mask you just created\n",
    "mask_nii_path = \"./results/segmentationd_native_fixed.nii.gz\"\n",
    "# Output filename\n",
    "output_path = \"./results/masked_ultrasound_native.nii.gz\"\n",
    "\n",
    "print(f\"ðŸ”¨ Creating Masked Ultrasound...\")\n",
    "\n",
    "# 2. LOAD DATA\n",
    "orig_nii = nib.load(original_nii_path)\n",
    "mask_nii = nib.load(mask_nii_path)\n",
    "\n",
    "orig_data = orig_nii.get_fdata()\n",
    "mask_data = mask_nii.get_fdata()\n",
    "\n",
    "# 3. APPLY THE MASK\n",
    "# Logic: Pixel * 1 = Pixel (Visible)\n",
    "#        Pixel * 0 = 0     (Black)\n",
    "# We ensure mask is binary (0 or 1) before multiplying\n",
    "binary_mask = (mask_data > 0).astype(orig_data.dtype)\n",
    "\n",
    "masked_volume = orig_data * binary_mask\n",
    "\n",
    "# 4. SAVE\n",
    "# We use the original header so it aligns perfectly in 3D space\n",
    "masked_nii = nib.Nifti1Image(\n",
    "    masked_volume, \n",
    "    orig_nii.affine, \n",
    "    orig_nii.header\n",
    ")\n",
    "\n",
    "nib.save(masked_nii, output_path)\n",
    "\n",
    "print(f\"âœ… Success! Saved to: {output_path}\")\n",
    "print(\"   Open this file in FreeView. You should see ONLY the fetal head.\")\n",
    "print(\"   The background will be completely black.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d472a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final pred_volume shape: (227, 149, 234)\n",
      "Original volume shape: (227, 149, 234)\n"
     ]
    }
   ],
   "source": [
    "print(\"Final pred_volume shape:\", pred_volume.shape)\n",
    "print(\"Original volume shape:\", vol_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1e0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First filtered slice: 72\n",
      "Last filtered slice: 228\n",
      "Blank slices from 72 to 233: [144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227]\n"
     ]
    }
   ],
   "source": [
    "#Get the first slice that survived filtering\n",
    "first_filtered_slice = min(filtered_measurements['slice_index'].unique())\n",
    "last_filtered_slice = max(filtered_measurements['slice_index'].unique())\n",
    "print(f\"First filtered slice: {first_filtered_slice}\")\n",
    "print(f\"Last filtered slice: {last_filtered_slice}\")\n",
    "#from the filtered slice to the center slice, get all the slices which are blank\n",
    "blank_slices = []\n",
    "for slice_idx in range(first_filtered_slice, last_filtered_slice + 1):\n",
    "    if np.sum(processed_volume[:, :, slice_idx]) == 0:\n",
    "        blank_slices.append(slice_idx)\n",
    "# Print the blank slices\n",
    "print(f\"Blank slices from {first_filtered_slice} to {vol_data.shape[2]-1}: {blank_slices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325c6bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw prediction saved to ./results/segmentation_RAW.nii.gz\n",
      "Processed prediction saved to ./FilteredRes/segmentation_fil.nii.gz\n",
      "Interpolated prediction saved to ./FilteredRes/segmentation_inter.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create results directories if they don't exist\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "os.makedirs('./FilteredRes', exist_ok=True)\n",
    "\n",
    "# Load original NIfTI for header info\n",
    "original_nii = nib.load(image_path)\n",
    "\n",
    "# Save raw prediction\n",
    "pred_nii = nib.Nifti1Image(pred_volume, original_nii.affine, original_nii.header)\n",
    "raw_filename = f'./results/segmentation_RAW.nii.gz'\n",
    "nib.save(pred_nii, raw_filename)\n",
    "print(f\"Raw prediction saved to {raw_filename}\")\n",
    "\n",
    "# Save processed prediction\n",
    "processed_nii = nib.Nifti1Image(processed_volume, original_nii.affine, original_nii.header)\n",
    "processed_filename = f'./FilteredRes/segmentation_fil.nii.gz'\n",
    "nib.save(processed_nii, processed_filename)\n",
    "print(f\"Processed prediction saved to {processed_filename}\")\n",
    "\n",
    "interpolated_volume = interpolate_blank_slices(image_path, processed_volume, blank_slices, predicted_masks, delta=1)\n",
    "# Save interpolated prediction\n",
    "interpolated_nii = nib.Nifti1Image(interpolated_volume, original_nii.affine, original_nii.header)\n",
    "interpolated_filename = f'./FilteredRes/segmentation_inter.nii.gz'\n",
    "nib.save(interpolated_nii, interpolated_filename)\n",
    "print(f\"Interpolated prediction saved to {interpolated_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0d0a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolated_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77147fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "nii = nib.load(image_path)\n",
    "vol = nii.get_fdata()\n",
    "H, W, Z = vol.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61576271",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interactive_infer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m model_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([model_input]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# --- inference ---\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m pred_mask_1024 \u001b[38;5;241m=\u001b[39m \u001b[43minteractive_infer\u001b[49m(model_input)  \u001b[38;5;66;03m# (1024, 1024)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# --- resize BACK to native grid ---\u001b[39;00m\n\u001b[1;32m     21\u001b[0m pred_native \u001b[38;5;241m=\u001b[39m resize(\n\u001b[1;32m     22\u001b[0m     pred_mask_1024,\n\u001b[1;32m     23\u001b[0m     (H, W),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     anti_aliasing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interactive_infer' is not defined"
     ]
    }
   ],
   "source": [
    "seg_native = np.zeros((H, W, Z), dtype=np.uint8)\n",
    "\n",
    "for z in range(Z):\n",
    "    slice_native = vol[:, :, z]\n",
    "\n",
    "    # --- model input copy (allowed to resize) ---\n",
    "    model_input = resize(\n",
    "        slice_native,\n",
    "        (1024, 1024),\n",
    "        preserve_range=True,\n",
    "        anti_aliasing=True\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    # if model expects RGB\n",
    "    model_input = np.stack([model_input]*3, axis=-1)\n",
    "\n",
    "    # --- inference ---\n",
    "    pred_mask_1024 = interactive_infer(model_input)  # (1024, 1024)\n",
    "\n",
    "    # --- resize BACK to native grid ---\n",
    "    pred_native = resize(\n",
    "        pred_mask_1024,\n",
    "        (H, W),\n",
    "        order=0,                 # IMPORTANT: nearest neighbor\n",
    "        preserve_range=True,\n",
    "        anti_aliasing=False\n",
    "    )\n",
    "\n",
    "    seg_native[:, :, z] = (pred_native > 0.5).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a0615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71855f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SHAPE DEBUG ===\n",
      "Original vol_data.shape: (227, 149, 234)\n",
      "pred_volume.shape:       (227, 149, 234)\n",
      "processed_volume.shape:  (227, 149, 234)\n",
      "interpolated_volume.shape: (227, 149, 234)\n",
      "===================\n"
     ]
    }
   ],
   "source": [
    "print(\"=== SHAPE DEBUG ===\")\n",
    "print(f\"Original vol_data.shape: {vol_data.shape}\")\n",
    "print(f\"pred_volume.shape:       {pred_volume.shape}\")\n",
    "print(f\"processed_volume.shape:  {processed_volume.shape}\")\n",
    "print(f\"interpolated_volume.shape: {interpolated_volume.shape}\")\n",
    "print(\"===================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1ecc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ORIGINAL NIfTI ===\n",
      "Data shape: (227, 149, 234)\n",
      "Voxel spacing (zooms): (0.3, 0.3, 0.3)\n",
      "Affine:\n",
      " [[ 0.30000001  0.         -0.         -0.        ]\n",
      " [ 0.          0.30000001 -0.         -0.        ]\n",
      " [ 0.          0.          0.30000001  0.        ]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Physical FOV (mm): [68.10000271 44.70000178 70.20000279]\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "nii = nib.load(image_path)\n",
    "\n",
    "print(\"=== ORIGINAL NIfTI ===\")\n",
    "print(\"Data shape:\", nii.shape)\n",
    "print(\"Voxel spacing (zooms):\", nii.header.get_zooms())\n",
    "print(\"Affine:\\n\", nii.affine)\n",
    "\n",
    "# Physical size (mm)\n",
    "zooms = nii.header.get_zooms()\n",
    "fov = np.array(nii.shape) * np.array(zooms)\n",
    "print(\"Physical FOV (mm):\", fov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10302f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER read_nifti ===\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (1024, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "vol_data = read_nifti(\n",
    "    image_path,\n",
    "    is_CT=False,\n",
    "    slice_idx=2,\n",
    "    HW_index=(0, 1)\n",
    ")\n",
    "\n",
    "print(\"\\n=== AFTER read_nifti ===\")\n",
    "print(\"Type:\", type(vol_data))\n",
    "print(\"Shape:\", vol_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf52d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SLICE FED TO MODEL ===\n",
      "Slice shape: (1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "slice_idx = 1\n",
    "slice_img = vol_data[:, :, slice_idx]\n",
    "\n",
    "print(\"\\n=== SLICE FED TO MODEL ===\")\n",
    "print(\"Slice shape:\", slice_img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db931c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original slice shape: (227, 149)\n"
     ]
    }
   ],
   "source": [
    "orig_slice = nii.get_fdata()[:, :, slice_idx]\n",
    "print(\"Original slice shape:\", orig_slice.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af29ce1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BEFORE SAVING PREDICTION ===\n",
      "pred_volume shape: (227, 149, 234)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== BEFORE SAVING PREDICTION ===\")\n",
    "print(\"pred_volume shape:\", pred_volume.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28397b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HEADER COMPARISON ===\n",
      "Orig shape: (227, 149, 234)\n",
      "Pred shape: (227, 149, 234)\n",
      "\n",
      "Voxel spacing:\n",
      "Orig zooms: (0.3, 0.3, 0.3)\n",
      "Pred zooms: (0.3, 0.3, 0.3)\n",
      "\n",
      "Affine difference:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "qform codes:\n",
      "Orig qform: 1\n",
      "Pred qform: 1\n",
      "\n",
      "sform codes:\n",
      "Orig sform: 1\n",
      "Pred sform: 1\n"
     ]
    }
   ],
   "source": [
    "orig = nib.load(image_path)\n",
    "pred = nib.load('/home/fetalusr1/Fetal-Head-Segmentation-master/FilteredRes/3rdsegmentation_inter.nii.gz')\n",
    "\n",
    "print(\"\\n=== HEADER COMPARISON ===\")\n",
    "print(\"Orig shape:\", orig.shape)\n",
    "print(\"Pred shape:\", pred.shape)\n",
    "\n",
    "print(\"\\nVoxel spacing:\")\n",
    "print(\"Orig zooms:\", orig.header.get_zooms())\n",
    "print(\"Pred zooms:\", pred.header.get_zooms())\n",
    "\n",
    "print(\"\\nAffine difference:\")\n",
    "print(orig.affine - pred.affine)\n",
    "\n",
    "print(\"\\nqform codes:\")\n",
    "print(\"Orig qform:\", orig.header['qform_code'])\n",
    "print(\"Pred qform:\", pred.header['qform_code'])\n",
    "\n",
    "print(\"\\nsform codes:\")\n",
    "print(\"Orig sform:\", orig.header['sform_code'])\n",
    "print(\"Pred sform:\", pred.header['sform_code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cac259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original zooms: (0.3, 0.3, 0.3)\n",
      "Original affine:\n",
      " [[ 0.30000001  0.         -0.         -0.        ]\n",
      " [ 0.          0.30000001 -0.         -0.        ]\n",
      " [ 0.          0.          0.30000001  0.        ]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Original sform code: 1\n",
      "Original qform code: 1\n",
      "âœ… RAW saved\n",
      "âœ… PROCESSED saved\n",
      "âœ… INTERPOLATED saved\n",
      "Saved zooms: (0.3, 0.3, 0.3)\n",
      "âœ… Zooms match!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "os.makedirs('./FilteredRes', exist_ok=True)\n",
    "\n",
    "original_nii = nib.load(image_path)\n",
    "original_data = original_nii.get_fdata()\n",
    "\n",
    "print(\"Original zooms:\", original_nii.header.get_zooms()[:3])\n",
    "print(\"Original affine:\\n\", original_nii.affine)\n",
    "\n",
    "# CORRECT SYNTAX[web:1]\n",
    "sform_aff, sform_code = original_nii.header.get_sform(coded=True)\n",
    "qform_aff, qform_code = original_nii.header.get_qform(coded=True)\n",
    "print(\"Original sform code:\", sform_code if sform_aff is not None else \"None\")\n",
    "print(\"Original qform code:\", qform_code if qform_aff is not None else \"None\")\n",
    "\n",
    "# RAW\n",
    "raw_mask = (pred_volume > 0).astype(np.float32)\n",
    "raw_output = original_data * raw_mask\n",
    "raw_nii = nib.Nifti1Image(raw_output, original_nii.affine, original_nii.header)\n",
    "raw_nii.update_header()\n",
    "nib.save(raw_nii, './results/segmentation_RAW_SYNC.nii.gz')\n",
    "print(\"âœ… RAW saved\")\n",
    "\n",
    "# PROCESSED\n",
    "processed_mask = (processed_volume > 0).astype(np.float32)\n",
    "processed_output = original_data * processed_mask\n",
    "processed_nii = nib.Nifti1Image(processed_output, original_nii.affine, original_nii.header)\n",
    "processed_nii.update_header()\n",
    "nib.save(processed_nii, './FilteredRes/segmentation_fil_SYNC.nii.gz')\n",
    "print(\"âœ… PROCESSED saved\")\n",
    "\n",
    "# INTERPOLATED (FINAL)\n",
    "interpolated_mask = (interpolated_volume > 0).astype(np.float32)\n",
    "interpolated_output = original_data * interpolated_mask\n",
    "interpolated_nii = nib.Nifti1Image(interpolated_output, original_nii.affine, original_nii.header)\n",
    "interpolated_nii.update_header()\n",
    "nib.save(interpolated_nii, './FilteredRes/segmentation_inter_SYNC.nii.gz')\n",
    "print(\"âœ… INTERPOLATED saved\")\n",
    "\n",
    "# VERIFY\n",
    "saved_zooms = interpolated_nii.header.get_zooms()[:3]\n",
    "print(\"Saved zooms:\", saved_zooms)\n",
    "print(\"âœ… Zooms match!\" if np.allclose(saved_zooms, original_nii.header.get_zooms()[:3]) else \"âŒ Zooms mismatch!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb0a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw mask voxels: 1484016\n",
      "âœ… Intern method saved. Voxels: 1713353.0\n"
     ]
    }
   ],
   "source": [
    "import scipy.ndimage as ndimage\n",
    "import nibabel as nib\n",
    "\n",
    "original_nii = nib.load(image_path)\n",
    "vol_data = original_nii.get_fdata()\n",
    "\n",
    "# INTERN'S EXACT METHOD on your interpolated_volume\n",
    "current_mask = interpolated_volume > 0  # Binary\n",
    "print(\"Raw mask voxels:\", np.sum(current_mask))\n",
    "\n",
    "# Aggressive dilation + smoothing\n",
    "structure = ndimage.generate_binary_structure(3, 1)\n",
    "dilated_mask = ndimage.binary_dilation(current_mask, structure=structure, iterations=3)\n",
    "dilated_mask = ndimage.median_filter(dilated_mask.astype(np.float32), size=3)\n",
    "\n",
    "final_output = vol_data * dilated_mask\n",
    "final_nii = nib.Nifti1Image(final_output.astype(np.float32), original_nii.affine, original_nii.header)\n",
    "nib.save(final_nii, './FilteredRes/final_METHOD.nii.gz')\n",
    "print(\"âœ… Intern method saved. Voxels:\", np.sum(dilated_mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0f3a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INTERN EXACT ===\n",
      "Processing volume with shape: (227, 149, 234)\n",
      "Reference slice: 117\n",
      "Found 260 regions across 234 slices\n",
      "Reference measurements - Major: 168.25, Minor: 95.13\n",
      "Filtering thresholds - Major: 109.37, Minor: 61.83\n",
      "After filtering: 106 regions in 106 slices\n",
      "Raw voxels: 1379858.0\n",
      "ðŸ›¡ï¸ Expanding mask buffer...\n",
      "Final voxels: 1813276.0\n",
      "âœ… INTERN_EXACT_0.35.nii.gz saved!\n"
     ]
    }
   ],
   "source": [
    "import scipy.ndimage as ndimage\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "# INTERN'S WORKFLOW (your variables already exist)\n",
    "vol_data = nib.load(image_path).get_fdata()\n",
    "original_nii = nib.load(image_path)\n",
    "\n",
    "print(\"=== INTERN EXACT ===\")\n",
    "\n",
    "# 1. INTERN'S FILTERING (looser threshold)\n",
    "processed_intern, _ = process_predicted_volume(pred_volume, threshold_factor=0.35, output_prefix='intern')\n",
    "\n",
    "# 2. INTERN'S DILATION (on RAW or lightly filtered)\n",
    "current_mask = (pred_volume > 0).astype(np.float32)  # Raw predictions!\n",
    "\n",
    "print(\"Raw voxels:\", np.sum(current_mask))\n",
    "\n",
    "# INTERN'S EXACT DILATION\n",
    "print(\"ðŸ›¡ï¸ Expanding mask buffer...\")\n",
    "structure = ndimage.generate_binary_structure(3, 1)\n",
    "dilated_mask = ndimage.binary_dilation(current_mask, structure=structure, iterations=3)\n",
    "\n",
    "# Heavy smoothing (intern style)\n",
    "final_mask = ndimage.median_filter(dilated_mask.astype(np.float32), size=3)\n",
    "\n",
    "print(\"Final voxels:\", np.sum(final_mask))\n",
    "\n",
    "# 3. Save\n",
    "final_output = vol_data * final_mask\n",
    "final_nii = nib.Nifti1Image(final_output.astype(np.float32), original_nii.affine, original_nii.header)\n",
    "final_nii.update_header()\n",
    "nib.save(final_nii, './FilteredRes/INTERN_EXACT_0.35.nii.gz')\n",
    "print(\"âœ… INTERN_EXACT_0.35.nii.gz saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f0e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"from inference_utils.inference import interactive_infer_image\\n\",\n",
      "\"from inference_utils.processing_utils import read_nifti\\n\",\n",
      "\"def inference_nifti(file_path, text_prompts, is_CT, slice_idx, site=None, HW_index=(0, 1), channel_idx=None, rotate=0):\\n\",\n",
      "\"    image = read_nifti(file_path, is_CT, slice_idx, site=site, HW_index=HW_index, channel_idx=channel_idx)\\n\",\n",
      "\"    pred_mask,out_prob = interactive_infer_image(model, Image.fromarray(image), text_prompts)\\n\",\n",
      "\"    segmented_images = get_segmentation_masks(image, pred_mask, text_prompts, rotate=rotate)\\n\",\n",
      "\"        image = read_nifti(image_path, is_CT=False, slice_idx=slice_idx, site=None, HW_index=(0, 1), channel_idx=None)\\n\",\n",
      "\"text_prompt = ['fetal head']\\n\",\n",
      "\"    image, pred_mask, segmentation_mask = inference_nifti(image_path, text_prompt, is_CT=False, slice_idx=slice_idx, site=None, rotate=0)\\n\",\n"
     ]
    }
   ],
   "source": [
    "# Find intern's exact inference code\n",
    "with open('EndToEndPipeline.ipynb', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Search for prediction parameters\n",
    "for line in content.split('\\n'):\n",
    "    if 'interactive_infer_image' in line or 'text_prompt' in line or 'read_nifti' in line:\n",
    "        print(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94aa132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… WHOLE_BRAIN.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import scipy.ndimage as ndimage\n",
    "import nibabel as nib\n",
    "\n",
    "raw_nii = nib.load('./results/segmentation_RAW.nii.gz')\n",
    "raw_data = raw_nii.get_fdata()\n",
    "vol_data = nib.load(image_path).get_fdata()\n",
    "\n",
    "# ULTRA-AGGRESSIVE (matches intern's \"whole brain\")\n",
    "mask = raw_data > 0.05  # Very low threshold\n",
    "mask = ndimage.binary_fill_holes(mask)  # Fill ALL internal holes\n",
    "dilated = ndimage.binary_dilation(mask, iterations=8)  # Massive expansion\n",
    "final_mask = ndimage.gaussian_filter(dilated.astype(float), sigma=2)\n",
    "\n",
    "# TEXTURE PRESERVATION (intern's key)\n",
    "final_output = vol_data * final_mask\n",
    "\n",
    "nib.save(nib.Nifti1Image(final_output, raw_nii.affine, raw_nii.header), \n",
    "         './FilteredRes/WHOLE_BRAIN.nii.gz')\n",
    "print(\"âœ… WHOLE_BRAIN.nii.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e6a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your raw voxels: 1304674\n",
      "Raw shape: (227, 149, 234)\n"
     ]
    }
   ],
   "source": [
    "# Compare raw prediction coverage\n",
    "print(\"Your raw voxels:\", np.sum(raw_data > 0))\n",
    "print(\"Raw shape:\", raw_data.shape)\n",
    "\n",
    "# If intern had bigger raw predictions â†’ Different model/inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0168cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fetalusr1/miniconda3/envs/fetal_project/lib/python3.8/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  max_size = (max_size + (stride - 1)) // stride * stride\n",
      "/home/fetalusr1/Fetal-Head-Segmentation-master/modeling/modules/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
      "/home/fetalusr1/miniconda3/envs/fetal_project/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "\n",
    "for slice_idx in range(Z):\n",
    "    # ---- native slice (DO NOT modify) ----\n",
    "    slice_native = vol_data[:, :, slice_idx]\n",
    "\n",
    "    # ---- model input (allowed to resize) ----\n",
    "    model_input = resize(\n",
    "        slice_native,\n",
    "        (1024, 1024),\n",
    "        preserve_range=True,\n",
    "        anti_aliasing=True\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    model_input = np.stack([model_input]*3, axis=-1)  # RGB\n",
    "    model_input_pil = Image.fromarray(model_input.astype(np.uint8))\n",
    "\n",
    "    # ---- inference ----\n",
    "    pred_masks, out_prob = interactive_infer_image(\n",
    "        model,\n",
    "        model_input_pil,\n",
    "        text_prompt\n",
    "    )\n",
    "\n",
    "    pred_mask_1024 = pred_masks[0]\n",
    "\n",
    "    # ---- resize BACK to native geometry ----\n",
    "    pred_native = resize(\n",
    "        pred_mask_1024,\n",
    "        (H, W),\n",
    "        order=0,                # nearest neighbor = NO distortion\n",
    "        preserve_range=True,\n",
    "        anti_aliasing=False\n",
    "    )\n",
    "\n",
    "    seg_native[:, :, slice_idx] = (pred_native > 0.5).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98898fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87808b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Volume Array Shape: (227, 149, 234)\n",
      "ðŸ“Š Volume Data Type: float64\n",
      "ðŸ“Š Max Value in Volume: 254.68303701335242\n",
      "ðŸ“ Original Zooms (Spacing): (0.3, 0.3, 0.3)\n",
      "ðŸ—ºï¸ Original Affine Matrix:\n",
      "[[ 0.30000001  0.         -0.         -0.        ]\n",
      " [ 0.          0.30000001 -0.         -0.        ]\n",
      " [ 0.          0.          0.30000001  0.        ]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "ðŸ§© Number of slices containing data: 157\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "# 1. Check the physical data shape\n",
    "print(f\"ðŸ“Š Volume Array Shape: {processed_volume.shape}\")\n",
    "print(f\"ðŸ“Š Volume Data Type: {processed_volume.dtype}\")\n",
    "print(f\"ðŸ“Š Max Value in Volume: {np.max(processed_volume)}\")\n",
    "\n",
    "# 2. Check the \"Ruler\" (Header)\n",
    "orig_nii = nib.load(image_path)\n",
    "print(f\"ðŸ“ Original Zooms (Spacing): {orig_nii.header.get_zooms()}\")\n",
    "print(f\"ðŸ—ºï¸ Original Affine Matrix:\\n{orig_nii.affine}\")\n",
    "\n",
    "# 3. Check for the \"Veto\" impact\n",
    "active_slices = np.where(np.sum(processed_volume, axis=(0, 1)) > 0)[0]\n",
    "print(f\"ðŸ§© Number of slices containing data: {len(active_slices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Axis Sync Complete.\n",
      "ðŸ§© Data Slices: 157/234\n",
      "ðŸ“ Open this file in Freeview: ./FilteredRes/final_3D_reconstruction_13_1.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Original for Reference\n",
    "orig_nii = nib.load(image_path)\n",
    "\n",
    "# 2. Check if we need to transpose the axes\n",
    "# If your data was processed as (H, W, Slices) but needs to be (Slices, H, W)\n",
    "# we force it into the correct 3D orientation\n",
    "final_data = processed_volume.astype(np.float32)\n",
    "\n",
    "# 3. Create the NIfTI object with the CORRECT Affine\n",
    "# This 'orig_nii.affine' is the key to stopping the rectangle\n",
    "corrected_nii = nib.Nifti1Image(final_data, orig_nii.affine, orig_nii.header)\n",
    "\n",
    "# 4. Force the zooms to 0.3mm to stop the 'Squish'\n",
    "corrected_nii.header.set_zooms((0.3, 0.3, 0.3))\n",
    "\n",
    "# 5. Save with a unique name\n",
    "final_path = f'./FilteredRes/final_3D_reconstruction_{FILE_ID}.nii.gz'\n",
    "nib.save(corrected_nii, final_path)\n",
    "\n",
    "print(f\"âœ… Axis Sync Complete.\")\n",
    "print(f\"ðŸ§© Data Slices: {len(active_slices)}/234\")\n",
    "print(f\"ðŸ“ Open this file in Freeview: {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c17ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ›¡ï¸ Expanding mask buffer...\n",
      "âœ… Anatomically Preserved Result Saved: ./FilteredRes/13_1_PRESERVED.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import scipy.ndimage as ndimage\n",
    "\n",
    "# 1. Expand the mask slightly (Dilation)\n",
    "# This adds a 2-3 pixel buffer around the head so we don't cut into the skull\n",
    "print(\"ðŸ›¡ï¸ Expanding mask buffer...\")\n",
    "structure = ndimage.generate_binary_structure(3, 1)\n",
    "# dilate by 2 iterations to ensure outer skull is included\n",
    "expanded_mask = ndimage.binary_dilation(interpolated_volume > 0, structure=structure, iterations=2)\n",
    "\n",
    "# 2. Smooth the expanded mask (to keep it natural, not jagged)\n",
    "expanded_mask = ndimage.median_filter(expanded_mask.astype(np.float32), size=3)\n",
    "\n",
    "# 3. Multiply by Original Intensity\n",
    "# Now we use the expanded mask so we don't strip too much\n",
    "final_output_vol = vol_data * expanded_mask\n",
    "\n",
    "# 4. Save with Header Correction (Fixing the Squished look)\n",
    "original_zooms = original_nii.header.get_zooms()\n",
    "improved_nii = nib.Nifti1Image(final_output_vol, original_nii.affine, original_nii.header)\n",
    "improved_nii.header.set_zooms(original_zooms)\n",
    "\n",
    "improved_filename = f'./FilteredRes/{FILE_ID}_PRESERVED.nii.gz'\n",
    "nib.save(improved_nii, improved_filename)\n",
    "print(f\"âœ… Anatomically Preserved Result Saved: {improved_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b812bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Array Shape saved: (227, 149, 234)\n",
      "ðŸ“ Spacing saved: (0.3, 0.3, 0.3)\n",
      "âœ… Check this in Freeview: ./FilteredRes/VERIFY_BASIC_3D.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "# Load original metadata\n",
    "orig_nii = nib.load(IMAGE_PATH)\n",
    "\n",
    "# Use your raw prediction volume directly\n",
    "# We convert to float32 to ensure compatibility with NIfTI\n",
    "data_to_save = pred_volume.astype(np.float32)\n",
    "\n",
    "# This is the most conservative way to save:\n",
    "# Use the EXACT affine and header from the original file\n",
    "basic_nii = nib.Nifti1Image(data_to_save, orig_nii.affine, orig_nii.header)\n",
    "\n",
    "# Force the spacing one last time to be sure\n",
    "basic_nii.header.set_zooms(orig_nii.header.get_zooms())\n",
    "\n",
    "save_path = './FilteredRes/VERIFY_BASIC_3D.nii.gz'\n",
    "nib.save(basic_nii, save_path)\n",
    "\n",
    "print(f\"ðŸ“Š Array Shape saved: {data_to_save.shape}\")\n",
    "print(f\"ðŸ“ Spacing saved: {basic_nii.header.get_zooms()}\")\n",
    "print(f\"âœ… Check this in Freeview: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c380fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nifti_with_synced_metadata(data, reference_nii, save_path, dtype=np.float32):\n",
    "    \"\"\"\n",
    "    Saves NIfTI with strictly synchronized affine, zooms, qform, and sform\n",
    "    from a reference ultrasound scan.\n",
    "    \"\"\"\n",
    "    # Create new image with reference affine\n",
    "    new_nii = nib.Nifti1Image(data.astype(dtype), reference_nii.affine)\n",
    "\n",
    "    # Copy header safely\n",
    "    new_header = reference_nii.header.copy()\n",
    "\n",
    "    # Explicitly enforce voxel spacing\n",
    "    new_header.set_zooms(reference_nii.header.get_zooms())\n",
    "\n",
    "    # Apply header\n",
    "    new_nii.header = new_header\n",
    "\n",
    "    # Lock spatial transforms\n",
    "    new_nii.set_qform(reference_nii.get_qform(), code=1)\n",
    "    new_nii.set_sform(reference_nii.get_sform(), code=1)\n",
    "\n",
    "    nib.save(new_nii, save_path)\n",
    "    print(f\"âœ… Saved with synchronized metadata: {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fetal_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
